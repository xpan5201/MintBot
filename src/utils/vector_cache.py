"""
å‘é‡æ£€ç´¢ç¼“å­˜ä¼˜åŒ–æ¨¡å— (v2.29.0)

æä¾›æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œä¼˜åŒ–ChromaDBå‘é‡æ£€ç´¢æ€§èƒ½ã€‚

ç‰¹æ€§:
- ğŸš€ æŸ¥è¯¢ç»“æœç¼“å­˜ - é¿å…é‡å¤æ£€ç´¢
- ğŸ§  åµŒå…¥å‘é‡ç¼“å­˜ - é¿å…é‡å¤è®¡ç®—
- ğŸ”„ LRUæ·˜æ±°ç­–ç•¥ - è‡ªåŠ¨ç®¡ç†ç¼“å­˜å¤§å°
- â° TTLè¿‡æœŸæœºåˆ¶ - è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜
- ğŸ“Š æ€§èƒ½ç»Ÿè®¡ - ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡
"""

import hashlib
import time
from collections import OrderedDict
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from src.utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    value: Any
    timestamp: float = field(default_factory=time.time)
    access_count: int = 0
    last_access: float = field(default_factory=time.time)


class VectorSearchCache:
    """
    å‘é‡æ£€ç´¢ç¼“å­˜

    ä½¿ç”¨LRUç­–ç•¥å’ŒTTLæœºåˆ¶ç®¡ç†ç¼“å­˜ï¼Œä¼˜åŒ–å‘é‡æ£€ç´¢æ€§èƒ½ã€‚
    """

    def __init__(
        self,
        max_size: int = 1000,
        ttl_seconds: int = 3600,
        enable_stats: bool = True,
    ):
        """
        åˆå§‹åŒ–å‘é‡æ£€ç´¢ç¼“å­˜

        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl_seconds: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            enable_stats: æ˜¯å¦å¯ç”¨ç»Ÿè®¡
        """
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.enable_stats = enable_stats

        # ä½¿ç”¨OrderedDictå®ç°LRU
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "expirations": 0,
            "total_queries": 0,
        }

        logger.info(f"å‘é‡æ£€ç´¢ç¼“å­˜åˆå§‹åŒ–: max_size={max_size}, ttl={ttl_seconds}s")

    def _generate_key(self, query: str, k: int, filter_dict: Optional[Dict] = None) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            filter_dict: è¿‡æ»¤æ¡ä»¶

        Returns:
            ç¼“å­˜é”®
        """
        # æ ‡å‡†åŒ–æŸ¥è¯¢ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ã€è½¬å°å†™ï¼‰
        import re
        normalized_query = re.sub(r'\s+', ' ', query.strip().lower())

        # ç”Ÿæˆå”¯ä¸€é”®
        key_parts = [normalized_query, str(k)]
        if filter_dict:
            key_parts.append(str(sorted(filter_dict.items())))

        key_string = "|".join(key_parts)

        # ä½¿ç”¨MD5ç”ŸæˆçŸ­é”®ï¼ˆé¿å…é”®è¿‡é•¿ï¼‰
        return hashlib.md5(key_string.encode()).hexdigest()

    def get(
        self,
        query: str,
        k: int,
        filter_dict: Optional[Dict] = None,
    ) -> Optional[List[Dict[str, Any]]]:
        """
        ä»ç¼“å­˜è·å–æ£€ç´¢ç»“æœ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            filter_dict: è¿‡æ»¤æ¡ä»¶

        Returns:
            ç¼“å­˜çš„æ£€ç´¢ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        if self.enable_stats:
            self._stats["total_queries"] += 1

        key = self._generate_key(query, k, filter_dict)

        if key not in self._cache:
            if self.enable_stats:
                self._stats["misses"] += 1
            return None

        entry = self._cache[key]

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if time.time() - entry.timestamp > self.ttl_seconds:
            del self._cache[key]
            if self.enable_stats:
                self._stats["expirations"] += 1
                self._stats["misses"] += 1
            logger.debug(f"ç¼“å­˜è¿‡æœŸ: {query[:30]}...")
            return None

        # æ›´æ–°è®¿é—®ä¿¡æ¯
        entry.access_count += 1
        entry.last_access = time.time()

        # ç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
        self._cache.move_to_end(key)

        if self.enable_stats:
            self._stats["hits"] += 1

        logger.debug(f"ç¼“å­˜å‘½ä¸­: {query[:30]}... (è®¿é—®æ¬¡æ•°: {entry.access_count})")
        return entry.value

    def put(
        self,
        query: str,
        k: int,
        results: List[Dict[str, Any]],
        filter_dict: Optional[Dict] = None,
    ) -> None:
        """
        å°†æ£€ç´¢ç»“æœæ”¾å…¥ç¼“å­˜

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            results: æ£€ç´¢ç»“æœ
            filter_dict: è¿‡æ»¤æ¡ä»¶
        """
        key = self._generate_key(query, k, filter_dict)

        # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œå¿…è¦æ—¶æ·˜æ±°æœ€æ—§çš„æ¡ç›®
        if len(self._cache) >= self.max_size:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆFIFOï¼‰
            oldest_key = next(iter(self._cache))
            del self._cache[oldest_key]
            if self.enable_stats:
                self._stats["evictions"] += 1
            logger.debug(f"ç¼“å­˜æ·˜æ±°: è¾¾åˆ°æœ€å¤§å¤§å° {self.max_size}")

        # æ·»åŠ æ–°æ¡ç›®
        self._cache[key] = CacheEntry(value=results)
        logger.debug(f"ç¼“å­˜æ·»åŠ : {query[:30]}... (å½“å‰å¤§å°: {len(self._cache)})")

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        logger.info("å‘é‡æ£€ç´¢ç¼“å­˜å·²æ¸…ç©º")

    def cleanup_expired(self) -> int:
        """
        æ¸…ç†è¿‡æœŸç¼“å­˜

        Returns:
            æ¸…ç†çš„æ¡ç›®æ•°
        """
        current_time = time.time()
        expired_keys = [
            key for key, entry in self._cache.items()
            if current_time - entry.timestamp > self.ttl_seconds
        ]

        for key in expired_keys:
            del self._cache[key]

        if expired_keys:
            if self.enable_stats:
                self._stats["expirations"] += len(expired_keys)
            logger.info(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")

        return len(expired_keys)

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.enable_stats:
            return {}

        total_queries = self._stats["total_queries"]
        hits = self._stats["hits"]

        hit_rate = (hits / total_queries) if total_queries > 0 else 0

        return {
            "cache_size": len(self._cache),
            "max_size": self.max_size,
            "hit_rate": hit_rate,
            "total_queries": total_queries,
            "hits": hits,
            "misses": self._stats["misses"],
            "evictions": self._stats["evictions"],
            "expirations": self._stats["expirations"],
        }

    def print_stats(self) -> None:
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        if not stats:
            logger.info("ç¼“å­˜ç»Ÿè®¡æœªå¯ç”¨")
            return

        logger.info("=" * 50)
        logger.info("å‘é‡æ£€ç´¢ç¼“å­˜ç»Ÿè®¡")
        logger.info("=" * 50)
        for key, value in stats.items():
            logger.info(f"{key}: {value}")
        logger.info("=" * 50)


class EmbeddingCache:
    """
    åµŒå…¥å‘é‡ç¼“å­˜

    ç¼“å­˜æ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚
    """

    def __init__(
        self,
        max_size: int = 500,
        ttl_seconds: int = 7200,
    ):
        """
        åˆå§‹åŒ–åµŒå…¥å‘é‡ç¼“å­˜

        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl_seconds: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds

        # ä½¿ç”¨OrderedDictå®ç°LRU
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()

        logger.info(f"åµŒå…¥å‘é‡ç¼“å­˜åˆå§‹åŒ–: max_size={max_size}, ttl={ttl_seconds}s")

    def _generate_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # æ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ã€è½¬å°å†™ï¼‰
        import re
        normalized_text = re.sub(r'\s+', ' ', text.strip().lower())
        # ä½¿ç”¨MD5ç”ŸæˆçŸ­é”®
        return hashlib.md5(normalized_text.encode()).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """è·å–åµŒå…¥å‘é‡"""
        key = self._generate_key(text)

        if key not in self._cache:
            return None

        entry = self._cache[key]

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if time.time() - entry.timestamp > self.ttl_seconds:
            del self._cache[key]
            return None

        # æ›´æ–°è®¿é—®ä¿¡æ¯
        entry.access_count += 1
        entry.last_access = time.time()

        # ç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
        self._cache.move_to_end(key)

        return entry.value

    def put(self, text: str, embedding: List[float]) -> None:
        """å­˜å‚¨åµŒå…¥å‘é‡"""
        key = self._generate_key(text)

        # æ£€æŸ¥ç¼“å­˜å¤§å°
        if len(self._cache) >= self.max_size:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = next(iter(self._cache))
            del self._cache[oldest_key]

        # æ·»åŠ æ–°æ¡ç›®
        self._cache[key] = CacheEntry(value=embedding)

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        logger.info("åµŒå…¥å‘é‡ç¼“å­˜å·²æ¸…ç©º")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_accesses = sum(entry.access_count for entry in self._cache.values())

        return {
            "cache_size": len(self._cache),
            "max_size": self.max_size,
            "total_accesses": total_accesses,
            "hit_rate": 0.0,  # EmbeddingCacheæ²¡æœ‰å‘½ä¸­ç‡ç»Ÿè®¡
        }


# å…¨å±€ç¼“å­˜å®ä¾‹
_vector_search_cache: Optional[VectorSearchCache] = None
_embedding_cache: Optional[EmbeddingCache] = None


def get_vector_search_cache() -> VectorSearchCache:
    """è·å–å…¨å±€å‘é‡æ£€ç´¢ç¼“å­˜å®ä¾‹"""
    global _vector_search_cache
    if _vector_search_cache is None:
        _vector_search_cache = VectorSearchCache()
    return _vector_search_cache


def get_embedding_cache() -> EmbeddingCache:
    """è·å–å…¨å±€åµŒå…¥å‘é‡ç¼“å­˜å®ä¾‹"""
    global _embedding_cache
    if _embedding_cache is None:
        _embedding_cache = EmbeddingCache()
    return _embedding_cache

