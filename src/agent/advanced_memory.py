"""
é«˜çº§è®°å¿†ç³»ç»Ÿæ¨¡å— (v2.30.43ä¼˜åŒ–ç‰ˆ)

å®ç°æ ¸å¿ƒè®°å¿†ã€æ—¥è®°åŠŸèƒ½å’ŒçŸ¥è¯†åº“ï¼ˆä¸–ç•Œä¹¦ï¼‰ã€‚
åŸºäº config.yaml ä¸­çš„é«˜çº§é…ç½®ã€‚

ä¼˜åŒ–å†…å®¹:
- ä½¿ç”¨ç»Ÿä¸€çš„ChromaDBåˆå§‹åŒ–å‡½æ•°ï¼Œæ¶ˆé™¤ä»£ç é‡å¤
- æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
- ä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨
- v2.30.32: å¢åŠ  LLM è¾…åŠ©æå–æƒ…æ„Ÿå’Œä¸»é¢˜ï¼Œæå‡å‡†ç¡®ç‡
- v2.30.32: å¢åŠ å…ƒæ•°æ®æå–ï¼ˆäººç‰©ã€åœ°ç‚¹ã€æ—¶é—´ã€äº‹ä»¶ï¼‰
- v2.30.40: é›†æˆæ··åˆæ£€ç´¢ç³»ç»Ÿï¼ˆå‘é‡ + BM25ï¼‰
- v2.30.40: å¢åŠ é‡æ’åºæœºåˆ¶å’ŒæŸ¥è¯¢æ‰©å±•
- v2.30.41: é›†æˆçŸ¥è¯†è´¨é‡ç®¡ç†ç³»ç»Ÿ
- v2.30.42: é›†æˆçŸ¥è¯†æ¨èç³»ç»Ÿï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨èã€ä¸»åŠ¨æ¨é€ã€ä½¿ç”¨ç»Ÿè®¡ï¼‰
- v2.30.43: é›†æˆçŸ¥è¯†å›¾è°±ç³»ç»Ÿï¼ˆå…³ç³»å»ºæ¨¡ã€çŸ¥è¯†æ¨ç†ã€å›¾è°±å¯è§†åŒ–ï¼‰
- v2.30.44: æ€§èƒ½ä¼˜åŒ–ï¼ˆå¤šçº§ç¼“å­˜ã€å¼‚æ­¥å¤„ç†ã€ChromaDB è°ƒä¼˜ï¼‰
"""

import time
import hashlib
import json
import re
import difflib
from threading import Lock
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

from src.config.settings import settings
from src.utils.logger import get_logger
from src.utils.chroma_helper import create_chroma_vectorstore, get_collection_count

logger = get_logger(__name__)

_CONTENT_CATEGORY_KEYWORDS = {
    "character": ("äººç‰©", "è§’è‰²", "äºº", "å¥¹", "ä»–", "åå­—"),
    "location": ("åœ°ç‚¹", "ä½ç½®", "åœ°æ–¹", "åœ¨", "ä½äº"),
    "item": ("ç‰©å“", "ä¸œè¥¿", "é“å…·", "è£…å¤‡"),
    "event": ("äº‹ä»¶", "å‘ç”Ÿ", "ç»å†", "æ•…äº‹"),
}
_CHINESE_KEYWORDS_PATTERN = re.compile(r"[\u4e00-\u9fa5]{2,4}")

# v2.30.40: å¯¼å…¥æ··åˆæ£€ç´¢ç³»ç»Ÿ
try:
    from src.agent.hybrid_retriever import HybridRetriever, Reranker, QueryExpander
    HAS_HYBRID_RETRIEVER = True
except Exception as exc:  # pragma: no cover - ç¯å¢ƒä¾èµ–å·®å¼‚
    HAS_HYBRID_RETRIEVER = False
    HybridRetriever = None  # type: ignore[assignment]
    Reranker = None  # type: ignore[assignment]
    QueryExpander = None  # type: ignore[assignment]
    logger.warning("æ··åˆæ£€ç´¢ç³»ç»Ÿå¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿå‘é‡æ£€ç´¢: %s", exc)

# v2.30.41: å¯¼å…¥çŸ¥è¯†è´¨é‡ç®¡ç†ç³»ç»Ÿ
try:
    from src.agent.knowledge_quality import KnowledgeQualityManager
    HAS_QUALITY_MANAGER = True
except Exception as exc:  # pragma: no cover - ç¯å¢ƒä¾èµ–å·®å¼‚
    HAS_QUALITY_MANAGER = False
    KnowledgeQualityManager = None  # type: ignore[assignment]
    logger.warning("çŸ¥è¯†è´¨é‡ç®¡ç†ç³»ç»Ÿå¯¼å…¥å¤±è´¥ï¼Œå°†è·³è¿‡è´¨é‡æ£€æŸ¥: %s", exc)

# v2.30.42: å¯¼å…¥çŸ¥è¯†æ¨èç³»ç»Ÿ
try:
    from src.agent.knowledge_recommender import (
        KnowledgeRecommender,
        ProactiveKnowledgePusher,
        KnowledgeUsageTracker,
    )
    HAS_RECOMMENDER = True
except Exception as exc:  # pragma: no cover - ç¯å¢ƒä¾èµ–å·®å¼‚
    HAS_RECOMMENDER = False
    KnowledgeRecommender = None  # type: ignore[assignment]
    ProactiveKnowledgePusher = None  # type: ignore[assignment]
    KnowledgeUsageTracker = None  # type: ignore[assignment]
    logger.warning("çŸ¥è¯†æ¨èç³»ç»Ÿå¯¼å…¥å¤±è´¥ï¼Œå°†è·³è¿‡æ¨èåŠŸèƒ½: %s", exc)

# v2.30.43: å¯¼å…¥çŸ¥è¯†å›¾è°±ç³»ç»Ÿ
try:
    from src.agent.knowledge_graph import KnowledgeGraph
    HAS_KNOWLEDGE_GRAPH = True
except Exception as exc:  # pragma: no cover - ç¯å¢ƒä¾èµ–å·®å¼‚
    HAS_KNOWLEDGE_GRAPH = False
    KnowledgeGraph = None  # type: ignore[assignment]
    logger.warning("çŸ¥è¯†å›¾è°±ç³»ç»Ÿå¯¼å…¥å¤±è´¥ï¼Œå°†è·³è¿‡å›¾è°±åŠŸèƒ½: %s", exc)

# v2.30.44: å¯¼å…¥æ€§èƒ½ä¼˜åŒ–å™¨
try:
    from src.agent.performance_optimizer import (
        MultiLevelCache,
        AsyncProcessor,
        ChromaDBOptimizer,
    )
    HAS_PERFORMANCE_OPTIMIZER = True
except ImportError:
    HAS_PERFORMANCE_OPTIMIZER = False
    logger.debug("æ€§èƒ½ä¼˜åŒ–å™¨æœªå®‰è£…ï¼Œå°†è·³è¿‡æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½")

# å°è¯•å¯¼å…¥ LangChain LLM
try:
    from langchain_openai import ChatOpenAI
    HAS_LANGCHAIN_LLM = True
except ImportError:
    HAS_LANGCHAIN_LLM = False
    logger.debug("langchain_openai æœªå®‰è£…ï¼ŒLLM è¾…åŠ©æå–åŠŸèƒ½å°†ä¸å¯ç”¨")


class CoreMemory:
    """
    æ ¸å¿ƒè®°å¿†ç³»ç»Ÿ

    å‚¨å­˜å…³äºç”¨æˆ·çš„é‡è¦ä¿¡æ¯ï¼ˆä½å€ã€çˆ±å¥½ã€å–œæ¬¢çš„ä¸œè¥¿ç­‰ï¼‰
    ä½¿ç”¨åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼ˆæ¨¡ç³Šæœç´¢ï¼‰
    """

    def __init__(self, persist_directory: Optional[str] = None, user_id: Optional[int] = None):
        """
        åˆå§‹åŒ–æ ¸å¿ƒè®°å¿† (v2.29.21ä¼˜åŒ–ç‰ˆ)

        Args:
            persist_directory: æŒä¹…åŒ–ç›®å½•
            user_id: ç”¨æˆ·IDï¼Œç”¨äºåˆ›å»ºç”¨æˆ·ç‰¹å®šçš„è®°å¿†è·¯å¾„
        """
        if not settings.agent.is_core_mem:
            logger.info("æ ¸å¿ƒè®°å¿†åŠŸèƒ½æœªå¯ç”¨")
            self.vectorstore = None
            return

        # æ”¯æŒç”¨æˆ·ç‰¹å®šè·¯å¾„
        if persist_directory:
            persist_dir = persist_directory
        elif user_id is not None:
            persist_dir = str(Path(settings.data_dir) / "users" / str(user_id) / "memory" / "core_memory")
        else:
            persist_dir = str(Path(settings.data_dir) / "memory" / "core_memory")

        # ä½¿ç”¨ç»Ÿä¸€çš„ChromaDBåˆå§‹åŒ–å‡½æ•°ï¼ˆv2.30.27: æ”¯æŒæœ¬åœ° embedding å’Œç¼“å­˜ï¼‰
        self.vectorstore = create_chroma_vectorstore(
            collection_name="core_memory",
            persist_directory=persist_dir,
            use_local_embedding=settings.use_local_embedding,
            enable_cache=settings.enable_embedding_cache,
        )

        if self.vectorstore:
            count = get_collection_count(self.vectorstore)
            logger.info(f"æ ¸å¿ƒè®°å¿†åˆå§‹åŒ–å®Œæˆï¼Œå·²æœ‰è®°å¿†: {count} æ¡")

    def add_core_memory(
        self,
        content: str,
        category: str = "general",
        importance: float = 1.0,
    ) -> None:
        """
        æ·»åŠ æ ¸å¿ƒè®°å¿†

        Args:
            content: è®°å¿†å†…å®¹
            category: è®°å¿†ç±»åˆ«ï¼ˆå¦‚ï¼špersonal_info, preferences, habitsï¼‰
            importance: é‡è¦æ€§ï¼ˆ0.0-1.0ï¼‰
        """
        if self.vectorstore is None:
            return

        metadata = {
            "category": category,
            "importance": importance,
            "timestamp": datetime.now().isoformat(),
            "type": "core_memory",
        }

        try:
            self.vectorstore.add_texts(
                texts=[content],
                metadatas=[metadata],
            )
            logger.info(f"æ·»åŠ æ ¸å¿ƒè®°å¿† [{category}]: {content[:50]}...")
        except Exception as e:
            logger.error(f"æ·»åŠ æ ¸å¿ƒè®°å¿†å¤±è´¥: {e}")

    def search_core_memories(
        self,
        query: str,
        k: int = 3,
        category: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢æ ¸å¿ƒè®°å¿†ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            category: ç­›é€‰ç±»åˆ«

        Returns:
            List[Dict]: è®°å¿†åˆ—è¡¨
        """
        if self.vectorstore is None:
            return []

        try:
            # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢
            results = self.vectorstore.similarity_search_with_score(
                query, k=k * 2  # å¤šè·å–ä¸€äº›ï¼Œåé¢è¿‡æ»¤
            )

            memories = []
            for doc, score in results:
                # v2.48.5: ç›¸ä¼¼åº¦è½¬æ¢ï¼ˆscore è¶Šå°è¶Šç›¸ä¼¼ï¼‰
                if (similarity := 1.0 - score) < settings.agent.mem_thresholds:
                    continue

                # ç±»åˆ«è¿‡æ»¤ï¼ˆv2.48.5: ä½¿ç”¨æµ·è±¡è¿ç®—ç¬¦ä¼˜åŒ–ï¼‰
                if category and (doc_category := doc.metadata.get("category")) != category:
                    continue

                memories.append({
                    "content": doc.page_content,
                    "similarity": similarity,
                    "metadata": doc.metadata,
                })

                if len(memories) >= k:
                    break

            logger.debug(f"æ ¸å¿ƒè®°å¿†æœç´¢: æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")
            return memories

        except Exception as e:
            logger.error(f"æ ¸å¿ƒè®°å¿†æœç´¢å¤±è´¥: {e}")
            return []

    def get_all_core_memories(self) -> List[str]:
        """è·å–æ‰€æœ‰æ ¸å¿ƒè®°å¿†"""
        if self.vectorstore is None:
            return []

        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            results = self.vectorstore.get()
            return results.get("documents", [])
        except Exception as e:
            logger.error(f"è·å–æ ¸å¿ƒè®°å¿†å¤±è´¥: {e}")
            return []


class DiaryMemory:
    """
    æ—¥è®°åŠŸèƒ½ï¼ˆé•¿æœŸè®°å¿†ï¼‰- v2.30.36 æ™ºèƒ½æ—¥è®°ç³»ç»Ÿ

    åƒäººç±»å†™æ—¥è®°ä¸€æ ·ï¼Œåªè®°å½•é‡è¦çš„äº‹æƒ…ï¼š
    1. é‡è¦å¯¹è¯ï¼ˆimportance >= 0.6ï¼‰
    2. æ¯æ—¥æ€»ç»“ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
    3. ç¾å¥½ç¬é—´ï¼ˆç‰¹æ®Šæƒ…æ„Ÿæ—¶åˆ»ï¼‰
    4. é‡è¦äº‹ä»¶ï¼ˆåŒ…å«äººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶çš„å¯¹è¯ï¼‰

    é•¿æœŸå‚¨å­˜å¯¹è¯ä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ—¶é—´ä¿¡æ¯è¿›è¡Œæ£€ç´¢
    ä¾‹å¦‚ï¼š"æ˜¨å¤©åšäº†ä»€ä¹ˆï¼Ÿ"ã€"ä¸¤å¤©å‰åƒçš„åˆé¥­æ˜¯ä»€ä¹ˆï¼Ÿ"
    """

    def __init__(self, persist_directory: Optional[str] = None, user_id: Optional[int] = None):
        """
        åˆå§‹åŒ–æ—¥è®°åŠŸèƒ½

        Args:
            persist_directory: æŒä¹…åŒ–ç›®å½•
            user_id: ç”¨æˆ·IDï¼Œç”¨äºåˆ›å»ºç”¨æˆ·ç‰¹å®šçš„è®°å¿†è·¯å¾„
        """
        if not settings.agent.long_memory:
            logger.info("æ—¥è®°åŠŸèƒ½æœªå¯ç”¨")
            self.vectorstore = None
            self.diary_file = None
            return

        # æ”¯æŒç”¨æˆ·ç‰¹å®šè·¯å¾„
        if persist_directory:
            persist_dir = persist_directory
        elif user_id is not None:
            persist_dir = str(Path(settings.data_dir) / "users" / str(user_id) / "memory" / "diary")
        else:
            persist_dir = str(Path(settings.data_dir) / "memory" / "diary")

        Path(persist_dir).mkdir(parents=True, exist_ok=True)

        # JSON æ–‡ä»¶å­˜å‚¨æ—¥è®°ï¼ˆä¾¿äºæ—¶é—´æ£€ç´¢ï¼‰
        self.diary_file = Path(persist_dir) / "diary.json"
        if not self.diary_file.exists():
            self.diary_file.write_text("[]", encoding="utf-8")

        # å‘é‡æ•°æ®åº“ï¼ˆç”¨äºè¯­ä¹‰æ£€ç´¢ï¼‰- ä½¿ç”¨ç»Ÿä¸€çš„åˆå§‹åŒ–å‡½æ•°ï¼ˆv2.30.27: æ”¯æŒæœ¬åœ° embedding å’Œç¼“å­˜ï¼‰
        self.vectorstore = create_chroma_vectorstore(
            collection_name="diary_memory",
            persist_directory=persist_dir,
            use_local_embedding=settings.use_local_embedding,
            enable_cache=settings.enable_embedding_cache,
        )

        if self.vectorstore:
            count = get_collection_count(self.vectorstore)
            logger.info(f"æ—¥è®°åŠŸèƒ½åˆå§‹åŒ–å®Œæˆï¼Œå·²æœ‰æ—¥è®°: {count} æ¡")

        # v2.30.32: åˆå§‹åŒ– LLMï¼ˆç”¨äºè¾…åŠ©æå–ï¼‰
        self.llm = None
        self.use_llm_extraction = getattr(settings.agent, "use_llm_extraction", False)  # é»˜è®¤å…³é—­ï¼Œé¿å…è¿‡åº¦è°ƒç”¨
        if self.use_llm_extraction and HAS_LANGCHAIN_LLM:
            try:
                self.llm = ChatOpenAI(
                    model=settings.llm.model,
                    temperature=0.0,  # ä½¿ç”¨ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
                    max_tokens=500,  # é™åˆ¶ token æ•°é‡
                    api_key=settings.llm.key,
                    base_url=settings.llm.api,
                )
                logger.info("LLM è¾…åŠ©æå–å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"LLM åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨å…³é”®è¯åŒ¹é…: {e}")
                self.use_llm_extraction = False

        # v2.30.34: æ€§èƒ½ä¼˜åŒ– - é¢„ç¼–è¯‘æƒ…æ„Ÿå’Œä¸»é¢˜å…³é”®è¯å­—å…¸ï¼ˆé¿å…æ¯æ¬¡è°ƒç”¨æ—¶é‡æ–°åˆ›å»ºï¼‰
        self._init_emotion_keywords()
        self._init_topic_keywords()

        # v2.30.36: æ™ºèƒ½æ—¥è®°ç³»ç»Ÿ - ä¸´æ—¶å¯¹è¯ç¼“å­˜ï¼ˆç”¨äºæ¯æ—¥æ€»ç»“ï¼‰
        self.daily_conversations = []  # å½“å¤©çš„æ‰€æœ‰å¯¹è¯
        self.last_summary_date = None  # ä¸Šæ¬¡æ€»ç»“çš„æ—¥æœŸ
        self.smart_diary_enabled = getattr(settings.agent, "smart_diary_enabled", True)
        self.diary_importance_threshold = getattr(settings.agent, "diary_importance_threshold", 0.6)
        self.daily_summary_enabled = getattr(settings.agent, "daily_summary_enabled", True)
        self.diary_daily_max_entries = max(1, int(getattr(settings.agent, "diary_daily_max_entries", 5)))
        self.diary_max_entries = max(1, int(getattr(settings.agent, "diary_max_entries", 500)))
        self.diary_max_days = max(1, int(getattr(settings.agent, "diary_max_days", 90)))
        self.diary_min_chars = max(1, int(getattr(settings.agent, "diary_min_chars", 10)))
        self.diary_min_interval_minutes = max(
            1, int(getattr(settings.agent, "diary_min_interval_minutes", 10))
        )
        self.diary_similarity_threshold = min(
            1.0, max(0.0, float(getattr(settings.agent, "diary_similarity_threshold", 0.9)))
        )
        self.diary_daily_highlights = max(1, int(getattr(settings.agent, "diary_daily_highlights", 3)))
        self._last_diary_ts: Optional[datetime] = None
        self._diary_cache: Optional[List[Dict[str, Any]]] = None
        self._diary_lock = Lock()

    def _init_emotion_keywords(self) -> None:
        """åˆå§‹åŒ–æƒ…æ„Ÿå…³é”®è¯å­—å…¸ (v2.30.34 æ€§èƒ½ä¼˜åŒ–)"""
        self.emotion_keywords = {
            "happy": {
                "å¼€å¿ƒ": 1.0, "é«˜å…´": 1.0, "å¿«ä¹": 1.0, "æ„‰å¿«": 1.0, "å–œæ‚¦": 1.0,
                "å¹¸ç¦": 1.0, "æ»¡è¶³": 0.8, "æ¬£å–œ": 1.0, "å–µ~": 0.6,
                "æ¬¢ä¹": 0.9, "æ¬£æ…°": 0.8, "èˆ’å¿ƒ": 0.8, "ç•…å¿«": 0.8,
                "å“ˆå“ˆ": 0.8, "å˜»å˜»": 0.8, "å‘µå‘µ": 0.6, "ç¬‘": 0.7,
                "å“ˆå“ˆå“ˆ": 0.9, "å˜¿å˜¿": 0.7, "å˜¿": 0.6, "ç¬‘äº†": 0.7,
                "å¥½": 0.5, "ä¸é”™": 0.6, "æ£’": 0.7, "èµ": 0.7,
                "ç¾": 0.6, "å¦™": 0.6, "çˆ½": 0.7, "èˆ’æœ": 0.6,
            },
            "sad": {
                "éš¾è¿‡": 1.0, "ä¼¤å¿ƒ": 1.0, "æ‚²ä¼¤": 1.0, "å¤±è½": 0.9, "æ²®ä¸§": 0.9,
                "éƒé—·": 0.8, "ä¸å¼€å¿ƒ": 1.0, "ç—›è‹¦": 1.0, "å¿ƒç—›": 1.0,
                "æ‚²ç—›": 1.0, "å¿§ä¼¤": 0.9, "å“€ä¼¤": 0.9, "å‡„å‡‰": 0.8,
                "å“­": 0.9, "å‘œå‘œ": 0.8, "æ³ª": 0.7, "çœ¼æ³ª": 0.8,
                "å“­äº†": 0.9, "æµæ³ª": 0.8, "æ³ªæ°´": 0.8, "å“­æ³£": 0.9,
                "ç´¯": 0.5, "ç–²æƒ«": 0.6, "æ— å¥ˆ": 0.6, "å¤±æœ›": 0.7,
                "ç»æœ›": 0.8, "å¿ƒé…¸": 0.7, "å§”å±ˆ": 0.7, "å­¤ç‹¬": 0.6,
            },
            "angry": {
                "ç”Ÿæ°”": 1.0, "æ„¤æ€’": 1.0, "æ¼ç«": 0.9, "æ°”æ„¤": 1.0, "è®¨åŒ": 0.8,
                "çƒ¦": 0.7, "å“¼": 0.6, "ç«å¤§": 0.9, "æ°”æ­»": 1.0,
                "æš´æ€’": 1.0, "å‘ç«": 0.9, "æ¼æ€’": 0.9, "æ„¤æ…¨": 0.9,
                "å¯æ¶": 0.8, "æ··è›‹": 0.9, "è¯¥æ­»": 0.9, "çƒ¦æ­»": 0.8,
                "çƒ¦äºº": 0.7, "è®¨åŒæ­»": 0.8, "æ°”äºº": 0.8, "å¯æ°”": 0.8,
            },
            "anxious": {
                "æ‹…å¿ƒ": 1.0, "ç„¦è™‘": 1.0, "ç´§å¼ ": 1.0, "ä¸å®‰": 0.9, "å®³æ€•": 1.0,
                "ææƒ§": 1.0, "å¿§è™‘": 0.9, "æ…Œ": 0.8, "æ€•": 0.7,
                "æƒŠæ…Œ": 0.9, "ææ…Œ": 0.9, "æƒ¶æ": 0.8, "æƒŠæ": 0.9,
                "ç´§è¿«": 0.7, "å‹åŠ›": 0.8, "å¿å¿‘": 0.9, "æ…Œå¼ ": 0.8,
                "æ…Œä¹±": 0.8, "ä¸çŸ¥æ‰€æª": 0.9, "æ‰‹è¶³æ— æª": 0.9, "å¿ƒæ…Œ": 0.8,
            },
            "excited": {
                "å…´å¥‹": 1.0, "æ¿€åŠ¨": 1.0, "æœŸå¾…": 0.9, "è¿«ä¸åŠå¾…": 1.0,
                "å¤ªæ£’äº†": 1.0, "å¥½æ£’": 0.9, "å‰å®³": 0.8, "ç‰›": 0.7,
                "ç›¼æœ›": 0.9, "æ¸´æœ›": 0.9, "å‘å¾€": 0.8, "æ†§æ†¬": 0.8,
                "å“‡": 0.7, "è€¶": 0.8, "èµ": 0.7, "å“‡å¡": 0.8, "å¤©å•Š": 0.7,
                "å¤ªå¥½äº†": 0.9, "çœŸæ£’": 0.8, "é…·": 0.7, "å¸…": 0.7,
                "æœŸç›¼": 0.6, "æƒ³": 0.4, "å¸Œæœ›": 0.5, "ç­‰ä¸åŠ": 0.8,
            },
        }

        self.negation_words = ["ä¸", "æ²¡", "æ— ", "æœª", "åˆ«", "è«", "å‹¿", "æ¯‹"]

        self.degree_words = {
            "è¶…çº§": 2.0, "éå¸¸": 2.0, "ç‰¹åˆ«": 2.0, "æå…¶": 2.0, "ååˆ†": 2.0,
            "å¤ª": 2.0, "æœ€": 2.0, "æ": 2.0, "è¶…": 1.8,
            "å·¨": 1.9, "æ— æ¯”": 2.0, "æ ¼å¤–": 1.9, "å¼‚å¸¸": 1.9,
            "å¾ˆ": 1.5, "æŒº": 1.5, "ç›¸å½“": 1.5, "é¢‡": 1.5, "è›®": 1.5,
            "å¤Ÿ": 1.4, "å®åœ¨": 1.5, "çœŸ": 1.5, "çœŸçš„": 1.5,
            "æ¯”è¾ƒ": 1.2, "è¿˜": 1.2, "ç¨å¾®": 0.8, "æœ‰ç‚¹": 0.8, "ç•¥": 0.8,
            "ç¨": 0.8, "äº›è®¸": 0.7, "ä¸€ç‚¹": 0.8, "ç‚¹": 0.7,
        }

        self.emotion_opposite = {
            "happy": "sad",
            "sad": "happy",
            "angry": "happy",
            "anxious": "happy",
            "excited": "neutral",
        }

        self.transition_words = ["ä½†æ˜¯", "ä½†", "å¯æ˜¯", "ä¸è¿‡", "ç„¶è€Œ", "å´", "åªæ˜¯", "å°±æ˜¯"]

    def _init_topic_keywords(self) -> None:
        """åˆå§‹åŒ–ä¸»é¢˜å…³é”®è¯å­—å…¸ (v2.30.34 æ€§èƒ½ä¼˜åŒ–)"""
        self.topic_keywords = {
            "work": {
                "å·¥ä½œ": 2.0, "é¡¹ç›®": 2.0, "ä»»åŠ¡": 1.8, "ä¼šè®®": 2.0,
                "åŒäº‹": 1.5, "è€æ¿": 1.8, "å…¬å¸": 1.8, "åŠ ç­": 2.0,
                "èŒä¸š": 1.5, "ä¸šåŠ¡": 1.5, "å®¢æˆ·": 1.5, "åˆåŒ": 1.8,
                "ä¸Šç­": 1.8, "ä¸‹ç­": 1.5, "åŠå…¬": 1.5, "èŒåœº": 1.5,
                "é¢†å¯¼": 1.8, "éƒ¨é—¨": 1.5, "å›¢é˜Ÿ": 1.5, "ç»©æ•ˆ": 1.8,
                "æŠ¥å‘Š": 1.5, "æ–‡æ¡£": 1.2, "é‚®ä»¶": 1.2, "ç”µè¯": 1.0,
            },
            "life": {
                "ç”Ÿæ´»": 2.0, "å®¶": 1.8, "å®¶äºº": 1.8, "çˆ¶æ¯": 1.5,
                "åƒé¥­": 1.5, "ç¡è§‰": 1.5, "ä¼‘æ¯": 1.5, "è´­ç‰©": 1.5,
                "åšé¥­": 1.5, "æ‰“æ‰«": 1.5, "æ´—è¡£": 1.2, "å®¶åŠ¡": 1.5,
                "æ—¥å¸¸": 1.5, "çäº‹": 1.2, "ç”Ÿæ´»çäº‹": 1.5,
            },
            "study": {
                "å­¦ä¹ ": 2.0, "è€ƒè¯•": 2.0, "ä½œä¸š": 1.8, "è¯¾ç¨‹": 1.8,
                "è€å¸ˆ": 1.5, "åŒå­¦": 1.5, "å­¦æ ¡": 1.8, "ä¸Šè¯¾": 1.8,
                "å¤ä¹ ": 1.8, "é¢„ä¹ ": 1.5, "ç¬”è®°": 1.5, "æ•™æ": 1.5,
                "çŸ¥è¯†": 1.5, "æŠ€èƒ½": 1.5, "åŸ¹è®­": 1.5, "è¯ä¹¦": 1.5,
            },
            "entertainment": {
                "å¨±ä¹": 2.0, "æ¸¸æˆ": 1.8, "ç”µå½±": 1.8, "éŸ³ä¹": 1.8,
                "çœ‹å‰§": 1.8, "è¿½å‰§": 1.8, "åŠ¨æ¼«": 1.5, "å°è¯´": 1.5,
                "ç©": 1.5, "é€›è¡—": 1.5, "æ—…æ¸¸": 1.8, "æ—…è¡Œ": 1.8,
                "èšä¼š": 1.5, "æ´¾å¯¹": 1.5, "å”±æ­Œ": 1.5, "è·³èˆ": 1.5,
            },
            "health": {
                "å¥åº·": 2.0, "èº«ä½“": 1.8, "ç”Ÿç—…": 1.8, "åŒ»é™¢": 1.8,
                "åŒ»ç”Ÿ": 1.5, "è¯": 1.5, "æ²»ç–—": 1.5, "æ£€æŸ¥": 1.5,
                "è¿åŠ¨": 1.8, "é”»ç‚¼": 1.8, "å¥èº«": 1.8, "è·‘æ­¥": 1.5,
                "é¥®é£Ÿ": 1.5, "è¥å…»": 1.5, "ç¡çœ ": 1.5, "ä¼‘æ¯": 1.2,
            },
            "relationship": {
                "æœ‹å‹": 2.0, "å‹æƒ…": 2.0, "æ‹çˆ±": 2.0, "çˆ±æƒ…": 2.0,
                "ç”·å‹": 1.8, "å¥³å‹": 1.8, "ä¼´ä¾£": 1.8, "å¯¹è±¡": 1.8,
                "å®¶äºº": 1.8, "äº²äºº": 1.8, "å…³ç³»": 1.5, "ç›¸å¤„": 1.5,
                "èŠå¤©": 1.2, "äº¤æµ": 1.2, "æ²Ÿé€š": 1.5, "ç†è§£": 1.2,
            },
        }

    def _extract_with_llm(self, content: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æå–å…ƒæ•°æ®ï¼ˆv2.30.32: æ–°å¢ï¼‰

        Args:
            content: æ—¥è®°å†…å®¹

        Returns:
            Dict[str, Any]: æå–çš„å…ƒæ•°æ®ï¼ŒåŒ…å« emotion, topic, people, location, time, event
        """
        if not self.llm:
            return {}

        try:
            prompt = f"""è¯·åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆä»¥JSONæ ¼å¼è¿”å›ï¼‰ï¼š

å¯¹è¯å†…å®¹ï¼š
{content}

è¯·æå–ï¼š
1. emotionï¼ˆæƒ…æ„Ÿï¼‰ï¼šhappyï¼ˆå¼€å¿ƒï¼‰ã€sadï¼ˆéš¾è¿‡ï¼‰ã€angryï¼ˆç”Ÿæ°”ï¼‰ã€anxiousï¼ˆç„¦è™‘ï¼‰ã€excitedï¼ˆå…´å¥‹ï¼‰ã€neutralï¼ˆä¸­æ€§ï¼‰ä¹‹ä¸€
2. topicï¼ˆä¸»é¢˜ï¼‰ï¼šworkï¼ˆå·¥ä½œï¼‰ã€lifeï¼ˆç”Ÿæ´»ï¼‰ã€studyï¼ˆå­¦ä¹ ï¼‰ã€entertainmentï¼ˆå¨±ä¹ï¼‰ã€healthï¼ˆå¥åº·ï¼‰ã€relationshipï¼ˆäººé™…å…³ç³»ï¼‰ã€otherï¼ˆå…¶ä»–ï¼‰ä¹‹ä¸€
3. peopleï¼ˆäººç‰©ï¼‰ï¼šå¯¹è¯ä¸­æåˆ°çš„äººç‰©åˆ—è¡¨ï¼ˆå¦‚ï¼šæœ‹å‹ã€å®¶äººã€åŒäº‹ç­‰ï¼‰
4. locationï¼ˆåœ°ç‚¹ï¼‰ï¼šå¯¹è¯ä¸­æåˆ°çš„åœ°ç‚¹ï¼ˆå¦‚ï¼šå…¬å¸ã€å®¶ã€é¤å…ç­‰ï¼‰
5. timeï¼ˆæ—¶é—´ï¼‰ï¼šå¯¹è¯ä¸­æåˆ°çš„æ—¶é—´ä¿¡æ¯ï¼ˆå¦‚ï¼šæ˜å¤©ã€ä¸‹å‘¨ã€æ˜¨å¤©ç­‰ï¼‰
6. eventï¼ˆäº‹ä»¶ï¼‰ï¼šå¯¹è¯ä¸­æåˆ°çš„é‡è¦äº‹ä»¶ï¼ˆå¦‚ï¼šä¼šè®®ã€è€ƒè¯•ã€æ—…è¡Œç­‰ï¼‰

è¿”å›æ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
    "emotion": "happy",
    "topic": "work",
    "people": ["åŒäº‹", "è€æ¿"],
    "location": "å…¬å¸",
    "time": "æ˜å¤©",
    "event": "é¡¹ç›®ä¼šè®®"
}}

å¦‚æœæŸé¡¹ä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·è¿”å› null æˆ–ç©ºåˆ—è¡¨ã€‚åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

            response = self.llm.invoke(prompt)
            result_text = response.content.strip()

            # æå– JSONï¼ˆå¯èƒ½è¢«åŒ…è£¹åœ¨ä»£ç å—ä¸­ï¼‰
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()

            result = json.loads(result_text)
            logger.debug(f"LLM æå–ç»“æœ: {result}")
            return result

        except Exception as e:
            logger.warning(f"LLM æå–å¤±è´¥: {e}")
            return {}

    def _should_save_as_diary(
        self,
        importance: float,
        emotion: str,
        people: List[str],
        location: Optional[str],
        event: Optional[str],
        content_len: int,
        existing_happy_count: int,
    ) -> tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜ä¸ºæ—¥è®°ï¼ˆv2.30.36: æ™ºèƒ½è¿‡æ»¤ï¼‰

        åƒäººç±»å†™æ—¥è®°ä¸€æ ·ï¼Œåªè®°å½•é‡è¦çš„äº‹æƒ…ï¼š
        1. é‡è¦å¯¹è¯ï¼ˆimportance >= 0.6ï¼‰
        2. ç¾å¥½ç¬é—´ï¼ˆhappy, excited æƒ…æ„Ÿï¼‰
        3. é‡è¦äº‹ä»¶ï¼ˆåŒ…å«äººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶ï¼‰
        4. ç‰¹æ®Šæƒ…æ„Ÿï¼ˆsad, angry ç­‰éœ€è¦è®°å½•çš„æƒ…æ„Ÿï¼‰

        Args:
            importance: é‡è¦æ€§è¯„åˆ†
            emotion: æƒ…æ„Ÿæ ‡ç­¾
            people: äººç‰©åˆ—è¡¨
            location: åœ°ç‚¹
            event: äº‹ä»¶

        Returns:
            tuple[bool, str]: (æ˜¯å¦ä¿å­˜, ä¿å­˜åŸå› )
        """
        reasons = []

        # 1. é‡è¦å¯¹è¯ï¼ˆimportance >= thresholdï¼‰
        if importance >= self.diary_importance_threshold:
            reasons.append(f"é‡è¦å¯¹è¯(é‡è¦æ€§:{importance:.2f})")

        # 2. ç¾å¥½ç¬é—´ï¼ˆhappy, excited æƒ…æ„Ÿï¼‰- éœ€æœ‰è¶³å¤Ÿé•¿åº¦æˆ–äº‹ä»¶ä¿¡æ¯
        if emotion in ["happy", "excited"] and existing_happy_count < 2 and (
            importance >= self.diary_importance_threshold
            or content_len >= max(30, self.diary_min_chars * 2)
            or people
            or location
            or event
        ):
            reasons.append(f"ç¾å¥½ç¬é—´({emotion})")

        # 3. é‡è¦äº‹ä»¶ï¼ˆåŒ…å«äººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶ï¼‰
        if (people and len(people) > 0) or location or event:
            event_info = []
            if people:
                event_info.append(f"äººç‰©:{','.join(people)}")
            if location:
                event_info.append(f"åœ°ç‚¹:{location}")
            if event:
                event_info.append(f"äº‹ä»¶:{event}")
            reasons.append(f"é‡è¦äº‹ä»¶({' '.join(event_info)})")

        # 4. ç‰¹æ®Šæƒ…æ„Ÿï¼ˆsad, angry ç­‰éœ€è¦è®°å½•çš„æƒ…æ„Ÿï¼‰
        if emotion in ["sad", "angry", "anxious"] and (
            importance >= 0.4 or content_len >= max(30, self.diary_min_chars * 2)
        ):
            reasons.append(f"ç‰¹æ®Šæƒ…æ„Ÿ({emotion})")

        # å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªæ¡ä»¶æ»¡è¶³ï¼Œå°±ä¿å­˜
        should_save = len(reasons) > 0
        reason_str = " | ".join(reasons) if reasons else "ä¸æ»¡è¶³æ—¥è®°æ¡ä»¶"

        return should_save, reason_str

    def _extract_diary_metadata(
        self,
        content: str,
        emotion: Optional[str],
        topic: Optional[str],
        importance: Optional[float],
        people: Optional[List[str]],
        location: Optional[str],
        time_info: Optional[str],
        event: Optional[str],
    ) -> Dict[str, Any]:
        """
        æå–æ—¥è®°å…ƒæ•°æ®ï¼ˆè¾…åŠ©æ–¹æ³•ï¼Œv2.48.4ï¼‰

        Args:
            content: æ—¥è®°å†…å®¹
            emotion: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            topic: ä¸»é¢˜æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            importance: é‡è¦æ€§è¯„åˆ†ï¼ˆå¯é€‰ï¼‰
            people: äººç‰©åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            location: åœ°ç‚¹ï¼ˆå¯é€‰ï¼‰
            time_info: æ—¶é—´ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            event: äº‹ä»¶ï¼ˆå¯é€‰ï¼‰

        Returns:
            Dict[str, Any]: æå–çš„å…ƒæ•°æ®
        """
        # v2.30.32: ä½¿ç”¨ LLM è¾…åŠ©æå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        llm_result = {}
        if self.use_llm_extraction and self.llm:
            llm_result = self._extract_with_llm(content)

        # v2.30.32: èåˆ LLM ç»“æœå’Œå…³é”®è¯åŒ¹é…ç»“æœ
        return {
            "emotion": emotion or llm_result.get("emotion") or self._extract_emotion(content),
            "topic": topic or llm_result.get("topic") or self._extract_topic(content),
            "importance": importance if importance is not None else self._calculate_importance(content),
            "people": people or llm_result.get("people") or [],
            "location": location or llm_result.get("location"),
            "time_info": time_info or llm_result.get("time"),
            "event": event or llm_result.get("event"),
        }

    def _save_diary_to_json(
        self,
        content: str,
        timestamp: datetime,
        metadata: Dict[str, Any],
    ) -> bool:
        """
        ä¿å­˜æ—¥è®°åˆ° JSON æ–‡ä»¶ï¼ˆè¾…åŠ©æ–¹æ³•ï¼Œv2.48.4ï¼‰

        Args:
            content: æ—¥è®°å†…å®¹
            timestamp: æ—¶é—´æˆ³
            metadata: å…ƒæ•°æ®å­—å…¸
        """
        try:
            diaries = self._get_diaries()
            # ç®€å•å»é‡ï¼šæœ€è¿‘ 50 æ¡å­˜åœ¨åŒæ ·å†…å®¹åˆ™è·³è¿‡
            recent_window = diaries[-50:] if len(diaries) > 50 else diaries
            for entry in reversed(recent_window):
                if entry.get("content") == content:
                    logger.debug("è·³è¿‡é‡å¤æ—¥è®°å†…å®¹ï¼ˆæœ€è¿‘çª—å£å·²å­˜åœ¨ï¼‰")
                    return False
                if self._is_similar(entry.get("content", ""), content):
                    logger.debug("è·³è¿‡é‡å¤æ—¥è®°å†…å®¹ï¼ˆç›¸ä¼¼åº¦è¿‡é«˜ï¼‰")
                    return False

            diary_entry = {
                "content": content,
                "timestamp": timestamp.isoformat(),
                "date": timestamp.strftime("%Y-%m-%d"),
                "time": timestamp.strftime("%H:%M:%S"),
                **metadata,  # å±•å¼€å…ƒæ•°æ®
            }
            diaries.append(diary_entry)

            diaries = self._prune_diaries(diaries)

            self._write_diaries(diaries)
            return True
        except Exception as e:
            logger.error(f"æ·»åŠ æ—¥è®°åˆ° JSON å¤±è´¥: {e}")
            return False

    def _prune_diaries(self, diaries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ ¹æ®é…ç½®è£å‰ªæ—¥è®°ï¼šæŒ‰æ—¶é—´æ’åºï¼Œé™åˆ¶å¤©æ•°ä¸æ€»æ¡æ•°ã€‚
        """
        if not diaries:
            return diaries

        # æŒ‰æ—¶é—´æ’åºï¼Œä¿è¯æœ€æ–°åœ¨å
        def _parse_ts(entry: Dict[str, Any]) -> float:
            ts = entry.get("timestamp") or entry.get("date")
            try:
                return datetime.fromisoformat(ts).timestamp() if ts else 0.0
            except Exception:
                return 0.0

        diaries = sorted(diaries, key=_parse_ts)

        # è¿‡æ»¤è¶…è¿‡ä¿ç•™å¤©æ•°çš„è€è®°å½•
        if self.diary_max_days:
            cutoff = datetime.now() - timedelta(days=self.diary_max_days)
            diaries = [
                entry
                for entry in diaries
                if (ts := entry.get("timestamp"))
                and self._safe_parse_datetime(ts) >= cutoff
            ] or diaries[-self.diary_max_entries :]

        # é™åˆ¶æœ€å¤§æ¡æ•°ï¼ˆä¿ç•™æœ€æ–°ï¼‰
        if self.diary_max_entries and len(diaries) > self.diary_max_entries:
            diaries = diaries[-self.diary_max_entries :]

        return diaries

    def _is_daily_cap_reached(self, date_str: str) -> bool:
        """
        åˆ¤æ–­æŸå¤©çš„æ—¥è®°æ¡æ•°æ˜¯å¦è¾¾åˆ°ä¸Šé™ã€‚
        """
        try:
            diaries = self._get_diaries()
            count = sum(1 for d in diaries if d.get("date") == date_str)
            return count >= self.diary_daily_max_entries
        except Exception as e:
            logger.warning(f"æ£€æŸ¥æ¯æ—¥æ—¥è®°ä¸Šé™å¤±è´¥: {e}")
            return False

    def _count_daily_emotion(self, date_str: str, target_emotions: Optional[set[str]] = None) -> int:
        """
        ç»Ÿè®¡æŸå¤©æŒ‡å®šæƒ…ç»ªçš„æ—¥è®°æ¡æ•°ã€‚
        """
        target_emotions = target_emotions or set()
        try:
            diaries = self._get_diaries()
            return sum(
                1
                for d in diaries
                if d.get("date") == date_str and d.get("emotion") in target_emotions
            )
        except Exception as e:
            logger.warning(f"ç»Ÿè®¡æ—¥è®°æƒ…ç»ªå¤±è´¥: {e}")
            return 0

    def _is_similar(self, existing: str, candidate: str) -> bool:
        """
        åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬æ˜¯å¦ç›¸ä¼¼ï¼ˆç”¨äºæ—¥è®°å»é‡ï¼‰ã€‚
        """
        if not existing or not candidate:
            return False
        existing_norm = existing.strip()
        candidate_norm = candidate.strip()
        if not existing_norm or not candidate_norm:
            return False
        ratio = difflib.SequenceMatcher(None, existing_norm, candidate_norm).ratio()
        return ratio >= self.diary_similarity_threshold

    def _get_diaries(self) -> List[Dict[str, Any]]:
        """
        è¯»å–æ—¥è®°åˆ—è¡¨ï¼Œå¸¦å†…å­˜ç¼“å­˜ï¼Œå‡å°‘é¢‘ç¹ I/Oã€‚
        """
        with self._diary_lock:
            if self._diary_cache is not None:
                return list(self._diary_cache)
            try:
                diaries = json.loads(self.diary_file.read_text(encoding="utf-8"))
                self._diary_cache = diaries
                return list(diaries)
            except Exception as e:
                logger.warning(f"è¯»å–æ—¥è®°ç¼“å­˜å¤±è´¥: {e}")
                self._diary_cache = []
                return []

    def _write_diaries(self, diaries: List[Dict[str, Any]]) -> None:
        """
        å†™å›æ—¥è®°æ–‡ä»¶å¹¶åˆ·æ–°ç¼“å­˜ã€‚
        """
        with self._diary_lock:
            self.diary_file.write_text(
                json.dumps(diaries, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )
            self._diary_cache = list(diaries)

    @staticmethod
    def _safe_parse_datetime(value: str) -> datetime:
        try:
            return datetime.fromisoformat(value)
        except Exception:
            return datetime.min

    def _save_diary_to_vectorstore(
        self,
        content: str,
        timestamp: datetime,
        metadata: Dict[str, Any],
    ) -> None:
        """
        ä¿å­˜æ—¥è®°åˆ°å‘é‡æ•°æ®åº“ï¼ˆè¾…åŠ©æ–¹æ³•ï¼Œv2.48.4ï¼‰

        Args:
            content: æ—¥è®°å†…å®¹
            timestamp: æ—¶é—´æˆ³
            metadata: å…ƒæ•°æ®å­—å…¸
        """
        vector_metadata = {
            "timestamp": timestamp.isoformat(),
            "date": timestamp.strftime("%Y-%m-%d"),
            "type": "diary",
            "emotion": metadata["emotion"],
            "topic": metadata["topic"],
            "importance": metadata["importance"],
            "people": json.dumps(metadata["people"], ensure_ascii=False) if metadata["people"] else "[]",
            "location": metadata.get("location") or "",
            "time_info": metadata.get("time_info") or "",
            "event": metadata.get("event") or "",
        }

        try:
            self.vectorstore.add_texts(
                texts=[content],
                metadatas=[vector_metadata],
            )
            # v2.30.32: å¢å¼ºæ—¥å¿—è¾“å‡º
            log_msg = (
                f"æ·»åŠ æ—¥è®°: {timestamp.strftime('%Y-%m-%d %H:%M')} - "
                f"æƒ…æ„Ÿ:{metadata['emotion']} ä¸»é¢˜:{metadata['topic']} é‡è¦æ€§:{metadata['importance']:.2f}"
            )
            if metadata["people"]:
                log_msg += f" äººç‰©:{','.join(metadata['people'])}"
            if metadata.get("location"):
                log_msg += f" åœ°ç‚¹:{metadata['location']}"
            if metadata.get("event"):
                log_msg += f" äº‹ä»¶:{metadata['event']}"
            log_msg += f" - {content[:50]}..."
            logger.info(log_msg)
        except Exception as e:
            logger.error(f"æ·»åŠ æ—¥è®°åˆ°å‘é‡åº“å¤±è´¥: {e}")

    def add_diary_entry(
        self,
        content: str,
        timestamp: Optional[datetime] = None,
        emotion: Optional[str] = None,
        topic: Optional[str] = None,
        importance: Optional[float] = None,
        people: Optional[List[str]] = None,
        location: Optional[str] = None,
        time_info: Optional[str] = None,
        event: Optional[str] = None,
        force_save: bool = False,  # v2.30.36: å¼ºåˆ¶ä¿å­˜ï¼ˆç”¨äºæ¯æ—¥æ€»ç»“ç­‰ï¼‰
    ) -> None:
        """
        æ·»åŠ æ—¥è®°æ¡ç›®ï¼ˆv2.30.36: æ™ºèƒ½è¿‡æ»¤ + æ¯æ—¥æ€»ç»“ï¼‰

        v2.48.4 é‡æ„ä¼˜åŒ–ï¼š
        - æå–è¾…åŠ©æ–¹æ³•ï¼Œå‡å°‘å‡½æ•°é•¿åº¦
        - æé«˜ä»£ç å¯ç»´æŠ¤æ€§

        åƒäººç±»å†™æ—¥è®°ä¸€æ ·ï¼Œåªè®°å½•é‡è¦çš„äº‹æƒ…ï¼š
        1. é‡è¦å¯¹è¯ï¼ˆimportance >= 0.6ï¼‰
        2. ç¾å¥½ç¬é—´ï¼ˆhappy, excited æƒ…æ„Ÿï¼‰
        3. é‡è¦äº‹ä»¶ï¼ˆåŒ…å«äººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶ï¼‰
        4. ç‰¹æ®Šæƒ…æ„Ÿï¼ˆsad, angry ç­‰éœ€è¦è®°å½•çš„æƒ…æ„Ÿï¼‰

        Args:
            content: æ—¥è®°å†…å®¹
            timestamp: æ—¶é—´æˆ³ï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¶é—´ï¼‰
            emotion: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆå¦‚ï¼šhappy, sad, neutralï¼‰
            topic: ä¸»é¢˜æ ‡ç­¾ï¼ˆå¦‚ï¼šå·¥ä½œã€ç”Ÿæ´»ã€å­¦ä¹ ï¼‰
            importance: é‡è¦æ€§è¯„åˆ†ï¼ˆ0.0-1.0ï¼‰
            people: äººç‰©åˆ—è¡¨ï¼ˆv2.30.32 æ–°å¢ï¼‰
            location: åœ°ç‚¹ï¼ˆv2.30.32 æ–°å¢ï¼‰
            time_info: æ—¶é—´ä¿¡æ¯ï¼ˆv2.30.32 æ–°å¢ï¼‰
            event: äº‹ä»¶ï¼ˆv2.30.32 æ–°å¢ï¼‰
            force_save: å¼ºåˆ¶ä¿å­˜ï¼ˆv2.30.36 æ–°å¢ï¼Œç”¨äºæ¯æ—¥æ€»ç»“ç­‰ï¼‰
        """
        if self.vectorstore is None or not self.diary_file:
            return

        if not content or len(content.strip()) < self.diary_min_chars:
            logger.debug("è·³è¿‡æ—¥è®°ä¿å­˜ï¼šå†…å®¹è¿‡çŸ­(<%d)", self.diary_min_chars)
            return

        timestamp = timestamp or datetime.now()
        date_str = timestamp.strftime("%Y-%m-%d")

        # å…ˆæå–å…ƒæ•°æ®ï¼Œåç»­è¿‡æ»¤/ç»Ÿè®¡éƒ½ä¼šç”¨åˆ°
        metadata = self._extract_diary_metadata(
            content, emotion, topic, importance, people, location, time_info, event
        )

        # é¢‘ç‡é™åˆ¶ï¼šé¿å…çŸ­æ—¶é—´å†…è¿ç»­è®°å½•
        if (
            not force_save
            and self._last_diary_ts is not None
            and (timestamp - self._last_diary_ts).total_seconds() < self.diary_min_interval_minutes * 60
        ):
            logger.debug(
                "è·³è¿‡æ—¥è®°ä¿å­˜ï¼šè·ç¦»ä¸Šæ¬¡æ—¥è®°ä¸è¶³ %d åˆ†é’Ÿ",
                self.diary_min_interval_minutes,
            )
            if self.daily_summary_enabled:
                self.daily_conversations.append({
                    "content": content,
                    "timestamp": timestamp,
                    **metadata,
                })
            return

        # v2.30.36: æ™ºèƒ½è¿‡æ»¤ - åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜ä¸ºæ—¥è®°
        if not force_save and self.smart_diary_enabled:
            should_save, reason = self._should_save_as_diary(
                importance=metadata["importance"],
                emotion=metadata["emotion"],
                people=metadata["people"],
                location=metadata.get("location"),
                event=metadata.get("event"),
                content_len=len(content),
                existing_happy_count=self._count_daily_emotion(date_str, target_emotions={"happy", "excited"}),
            )
            if not should_save:
                logger.debug("è·³è¿‡æ—¥è®°ä¿å­˜: %s - %.30s...", reason, content)
                # æ·»åŠ åˆ°ä¸´æ—¶å¯¹è¯ç¼“å­˜ï¼ˆç”¨äºæ¯æ—¥æ€»ç»“ï¼‰
                if self.daily_summary_enabled:
                    self.daily_conversations.append({
                        "content": content,
                        "timestamp": timestamp,
                        **metadata,
                    })
                return
            else:
                logger.info("ä¿å­˜æ—¥è®°: %s", reason)

        # æ¯æ—¥æ¡æ•°ä¸Šé™
        if self._is_daily_cap_reached(date_str):
            logger.debug("è·³è¿‡æ—¥è®°ä¿å­˜ï¼šå·²è¾¾åˆ°æ¯æ—¥ä¸Šé™ %d", self.diary_daily_max_entries)
            if self.daily_summary_enabled:
                self.daily_conversations.append({
                    "content": content,
                    "timestamp": timestamp,
                    **metadata,
                })
            return

        # v2.48.4: ä½¿ç”¨è¾…åŠ©æ–¹æ³•ä¿å­˜æ—¥è®°
        saved = self._save_diary_to_json(content, timestamp, metadata)
        if saved:
            self._save_diary_to_vectorstore(content, timestamp, metadata)
            self._last_diary_ts = timestamp
            if self.daily_summary_enabled:
                self.daily_conversations.append({
                    "content": content,
                    "timestamp": timestamp,
                    **metadata,
                })

    def generate_daily_summary(self, force: bool = False) -> Optional[str]:
        """
        ç”Ÿæˆæ¯æ—¥æ€»ç»“ï¼ˆv2.30.36: æ™ºèƒ½æ—¥è®°ç³»ç»Ÿï¼‰

        åœ¨ä¸€å¤©ç»“æŸæ—¶ï¼ˆæˆ–æ‰‹åŠ¨è§¦å‘ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆä»Šå¤©çš„å¯¹è¯æ€»ç»“å¹¶ä¿å­˜ä¸ºæ—¥è®°

        Args:
            force: å¼ºåˆ¶ç”Ÿæˆæ€»ç»“ï¼ˆå³ä½¿ä»Šå¤©å·²ç»ç”Ÿæˆè¿‡ï¼‰

        Returns:
            Optional[str]: ç”Ÿæˆçš„æ€»ç»“å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰å¯¹è¯åˆ™è¿”å› None
        """
        if self.vectorstore is None or not self.diary_file:
            return None

        today = datetime.now().date()

        # æ£€æŸ¥æ˜¯å¦å·²ç»ç”Ÿæˆè¿‡ä»Šå¤©çš„æ€»ç»“
        if not force and self.last_summary_date == today:
            logger.debug("ä»Šå¤©å·²ç»ç”Ÿæˆè¿‡æ€»ç»“ï¼Œè·³è¿‡")
            return None

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹è¯éœ€è¦æ€»ç»“
        if not self.daily_conversations:
            logger.debug("ä»Šå¤©æ²¡æœ‰å¯¹è¯éœ€è¦æ€»ç»“")
            return None

        # ç»Ÿè®¡ä»Šå¤©çš„å¯¹è¯
        total_conversations = len(self.daily_conversations)
        emotion_counts = {}
        topic_counts = {}
        avg_importance = 0.0

        for conv in self.daily_conversations:
            emotion = conv.get("emotion", "neutral")
            topic = conv.get("topic", "other")
            importance = conv.get("importance", 0.0)

            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            topic_counts[topic] = topic_counts.get(topic, 0) + 1
            avg_importance += importance

        avg_importance /= total_conversations if total_conversations > 0 else 1

        # ç”Ÿæˆæ€»ç»“
        emotion_summary = ", ".join([f"{k}:{v}æ¬¡" for k, v in emotion_counts.items()])
        topic_summary = ", ".join([f"{k}:{v}æ¬¡" for k, v in topic_counts.items()])
        highlights = sorted(
            self.daily_conversations,
            key=lambda x: (
                x.get("importance", 0.0),
                x.get("timestamp", datetime.min),
            ),
            reverse=True,
        )[: self.diary_daily_highlights]

        highlight_lines = []
        for h in highlights:
            ts = h.get("timestamp")
            ts_str = ts.strftime("%H:%M") if isinstance(ts, datetime) else ""
            emo = h.get("emotion", "neutral")
            topic = h.get("topic", "other")
            snippet = h.get("content", "")[:60]
            highlight_lines.append(f"- {ts_str} [{emo}/{topic}] {snippet}")

        summary = (
            f"ã€{today.strftime('%Yå¹´%mæœˆ%dæ—¥')} æ¯æ—¥æ€»ç»“ã€‘\n\n"
            f"ä»Šå¤©å’Œä¸»äººä¸€å…±èŠäº† {total_conversations} æ¬¡å¤©ã€‚\n\n"
            f"æƒ…æ„Ÿåˆ†å¸ƒ: {emotion_summary}\n"
            f"è¯é¢˜åˆ†å¸ƒ: {topic_summary}\n"
            f"å¹³å‡é‡è¦æ€§: {avg_importance:.2f}\n\n"
            f"ä»Šå¤©çš„é«˜å…‰æ—¶åˆ»ï¼š\n" + ("\n".join(highlight_lines) if highlight_lines else "ï¼ˆæš‚æ— ï¼‰") + "\n\n"
            f"ä»Šå¤©æ˜¯ç¾å¥½çš„ä¸€å¤©ï¼ŒæœŸå¾…æ˜å¤©ç»§ç»­å’Œä¸»äººèŠå¤©å–µ~ ğŸ’•"
        )

        # ä¿å­˜æ€»ç»“ä¸ºæ—¥è®°ï¼ˆå¼ºåˆ¶ä¿å­˜ï¼‰
        self.add_diary_entry(
            content=summary,
            timestamp=datetime.now(),
            emotion="happy",
            topic="life",
            importance=0.8,  # æ¯æ—¥æ€»ç»“éƒ½æ˜¯é‡è¦çš„
            force_save=True,  # å¼ºåˆ¶ä¿å­˜
        )

        # æ¸…ç©ºä»Šå¤©çš„å¯¹è¯ç¼“å­˜
        self.daily_conversations = []
        self.last_summary_date = today

        logger.info(f"ç”Ÿæˆæ¯æ—¥æ€»ç»“: {total_conversations} æ¬¡å¯¹è¯")
        return summary

    def _extract_emotion(self, content: str) -> str:
        """
        æå–æƒ…æ„Ÿæ ‡ç­¾ï¼ˆv2.30.34: æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨é¢„ç¼–è¯‘çš„å…³é”®è¯å­—å…¸ï¼‰

        Args:
            content: æ—¥è®°å†…å®¹

        Returns:
            str: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆhappy, sad, angry, anxious, excited, neutralï¼‰
        """
        # v2.30.31: æ··åˆæƒ…æ„Ÿè¯†åˆ« - è¯†åˆ«"è™½ç„¶...ä½†æ˜¯..."å¥å¼
        # æ£€æŸ¥æ˜¯å¦æœ‰è½¬æŠ˜è¯
        has_transition = False
        transition_pos = -1
        for word in self.transition_words:
            pos = content.find(word)
            if pos != -1:
                has_transition = True
                transition_pos = max(transition_pos, pos + len(word))

        # å¦‚æœæœ‰è½¬æŠ˜è¯ï¼Œåªåˆ†æè½¬æŠ˜è¯åé¢çš„å†…å®¹
        if has_transition and transition_pos > 0:
            content = content[transition_pos:]

        # v2.30.34: ä½¿ç”¨é¢„ç¼–è¯‘çš„å…³é”®è¯å­—å…¸ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        emotion_keywords = self.emotion_keywords
        negation_words = self.negation_words
        degree_words = self.degree_words
        emotion_opposite = self.emotion_opposite

        # è®¡ç®—æ¯ç§æƒ…æ„Ÿçš„åŠ æƒå¾—åˆ†
        emotion_scores = {}
        for emotion, keywords in emotion_keywords.items():
            total_score = 0.0

            for keyword, base_weight in keywords.items():
                if keyword not in content:
                    continue

                # æ‰¾åˆ°å…³é”®è¯çš„æ‰€æœ‰ä½ç½®
                start = 0
                while True:
                    pos = content.find(keyword, start)
                    if pos == -1:
                        break

                    # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰å¦å®šè¯ï¼ˆå‰3ä¸ªå­—ç¬¦å†…ï¼‰
                    has_negation = False
                    check_start = max(0, pos - 3)
                    prefix = content[check_start:pos]
                    for neg_word in negation_words:
                        if neg_word in prefix:
                            has_negation = True
                            break

                    # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰ç¨‹åº¦å‰¯è¯ï¼ˆå‰5ä¸ªå­—ç¬¦å†…ï¼Œå¢åŠ èŒƒå›´ä»¥æ”¯æŒ"éå¸¸"ï¼‰
                    degree_multiplier = 1.0
                    check_start = max(0, pos - 5)
                    prefix = content[check_start:pos]
                    for degree_word, multiplier in degree_words.items():
                        if degree_word in prefix:
                            degree_multiplier = max(degree_multiplier, multiplier)

                    # è®¡ç®—æœ€ç»ˆå¾—åˆ†
                    score = base_weight * degree_multiplier

                    # å¦‚æœæœ‰å¦å®šè¯ï¼Œæƒ…æ„Ÿåè½¬
                    if has_negation:
                        # å°†å¾—åˆ†æ·»åŠ åˆ°ç›¸åçš„æƒ…æ„Ÿ
                        opposite_emotion = emotion_opposite.get(emotion, "neutral")
                        if opposite_emotion not in emotion_scores:
                            emotion_scores[opposite_emotion] = 0.0
                        emotion_scores[opposite_emotion] += score * 0.8  # å¦å®šåçš„æƒ…æ„Ÿå¼ºåº¦ç•¥å‡
                    else:
                        # æ­£å¸¸æƒ…æ„Ÿ
                        total_score += score

                    start = pos + len(keyword)

            if total_score > 0:
                emotion_scores[emotion] = emotion_scores.get(emotion, 0.0) + total_score

        # è¿”å›å¾—åˆ†æœ€é«˜çš„æƒ…æ„Ÿï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å› neutral
        if emotion_scores:
            return max(emotion_scores, key=emotion_scores.get)
        return "neutral"

    def _extract_topic(self, content: str) -> str:
        """
        æå–ä¸»é¢˜æ ‡ç­¾ï¼ˆv2.30.34: æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨é¢„ç¼–è¯‘çš„å…³é”®è¯å­—å…¸ï¼‰

        Args:
            content: æ—¥è®°å†…å®¹

        Returns:
            str: ä¸»é¢˜æ ‡ç­¾ï¼ˆwork, life, study, entertainment, health, relationship, otherï¼‰
        """
        # v2.30.34: ä½¿ç”¨é¢„ç¼–è¯‘çš„å…³é”®è¯å­—å…¸ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        topic_keywords = self.topic_keywords

        # ä¸»é¢˜ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        topic_priority = {
            "relationship": 1,  # æœ€é«˜ä¼˜å…ˆçº§
            "health": 2,
            "work": 3,
            "study": 4,
            "entertainment": 5,
            "life": 6,  # æœ€ä½ä¼˜å…ˆçº§
        }

        # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„åŠ æƒå¾—åˆ†
        topic_scores = {}
        for topic, keywords in topic_keywords.items():
            total_score = 0.0
            for keyword, weight in keywords.items():
                if keyword in content:
                    # è®¡ç®—å…³é”®è¯å‡ºç°æ¬¡æ•°
                    count = content.count(keyword)
                    total_score += weight * count

            if total_score > 0:
                # åº”ç”¨ä¼˜å…ˆçº§åŠ æˆï¼ˆä¼˜å…ˆçº§è¶Šé«˜ï¼ŒåŠ æˆè¶Šå¤§ï¼‰
                priority = topic_priority.get(topic, 10)
                priority_bonus = 1.0 + (10 - priority) * 0.1  # ä¼˜å…ˆçº§1åŠ æˆ1.9ï¼Œä¼˜å…ˆçº§6åŠ æˆ1.4
                topic_scores[topic] = total_score * priority_bonus

        # è¿”å›å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å› other
        if topic_scores:
            return max(topic_scores, key=topic_scores.get)
        return "other"

    def _calculate_importance(self, content: str) -> float:
        """
        è®¡ç®—é‡è¦æ€§è¯„åˆ†ï¼ˆv2.30.30: å¢å¼ºç‰ˆ - æ›´ç²¾ç»†çš„æƒé‡å’Œæ›´å¤šå…³é”®è¯ï¼‰

        Args:
            content: æ—¥è®°å†…å®¹

        Returns:
            float: é‡è¦æ€§è¯„åˆ†ï¼ˆ0.0-1.0ï¼‰
        """
        # åŸºç¡€åˆ†æ•°ï¼šåŸºäºå†…å®¹é•¿åº¦ï¼ˆè°ƒæ•´å…¬å¼ï¼Œè®©é•¿æ¶ˆæ¯å¾—åˆ†æ›´é«˜ï¼‰
        # ä½¿ç”¨å¯¹æ•°å‡½æ•°ï¼Œè®©é•¿åº¦å½±å“æ›´å¹³æ»‘
        import math
        length_score = min(math.log(len(content) + 1) / 10, 0.3)  # æœ€å¤š0.3åˆ†

        # é‡è¦æ€§å…³é”®è¯ï¼ˆå¸¦æƒé‡ï¼‰
        important_keywords = {
            # æé«˜é‡è¦æ€§ï¼ˆæƒé‡ 0.25ï¼‰
            "ç´§æ€¥": 0.25, "ä¸¥é‡": 0.25, "å±é™©": 0.25, "è­¦å‘Š": 0.25,
            "ç«‹å³": 0.25, "é©¬ä¸Š": 0.25, "èµ¶ç´§": 0.25,
            # é«˜é‡è¦æ€§ï¼ˆæƒé‡ 0.20ï¼‰
            "é‡è¦": 0.20, "å…³é”®": 0.20, "å¿…é¡»": 0.20, "ä¸€å®š": 0.20,
            "åŠ¡å¿…": 0.20, "åƒä¸‡": 0.20, "åˆ‡è®°": 0.20,
            # ä¸­é‡è¦æ€§ï¼ˆæƒé‡ 0.15ï¼‰
            "è®°ä½": 0.15, "æé†’": 0.15, "åˆ«å¿˜": 0.15, "æ³¨æ„": 0.15,
            "å°å¿ƒ": 0.15, "å½“å¿ƒ": 0.15, "ç•™æ„": 0.15,
            # ä½é‡è¦æ€§ï¼ˆæƒé‡ 0.10ï¼‰
            "éœ€è¦": 0.10, "åº”è¯¥": 0.10, "æœ€å¥½": 0.10, "å»ºè®®": 0.10,
            "å¸Œæœ›": 0.08, "æƒ³è¦": 0.08, "æ‰“ç®—": 0.08,
        }

        # å…³é”®è¯åŠ åˆ†ï¼ˆç´¯åŠ æ‰€æœ‰åŒ¹é…çš„å…³é”®è¯æƒé‡ï¼‰
        keyword_score = 0.0
        for keyword, weight in important_keywords.items():
            if keyword in content:
                # è®¡ç®—å…³é”®è¯å‡ºç°æ¬¡æ•°
                count = content.count(keyword)
                keyword_score += weight * count

        # é™åˆ¶å…³é”®è¯å¾—åˆ†ä¸Šé™
        keyword_score = min(keyword_score, 0.6)  # æœ€å¤š0.6åˆ†

        # ç‰¹æ®Šäº‹ä»¶åŠ åˆ†
        special_events = {
            # æ—¶é—´ç›¸å…³ï¼ˆæƒé‡ 0.15ï¼‰
            "æ˜å¤©": 0.15, "ä»Šå¤©": 0.10, "ä¸‹å‘¨": 0.15, "ä¸‹æœˆ": 0.15,
            "æˆªæ­¢": 0.20, "æœŸé™": 0.20, "æ—¥æœŸ": 0.10,
            # äººç‰©ç›¸å…³ï¼ˆæƒé‡ 0.10ï¼‰
            "ä¼šè®®": 0.15, "é¢è¯•": 0.20, "çº¦ä¼š": 0.15, "èšä¼š": 0.10,
            "ç”Ÿæ—¥": 0.15, "çºªå¿µæ—¥": 0.15,
            # äº‹ä»¶ç›¸å…³ï¼ˆæƒé‡ 0.10ï¼‰
            "è€ƒè¯•": 0.20, "æ¯”èµ›": 0.15, "æ¼”å‡º": 0.15, "æ´»åŠ¨": 0.10,
            "é¡¹ç›®": 0.15, "ä»»åŠ¡": 0.12,
        }

        # ç‰¹æ®Šäº‹ä»¶åŠ åˆ†
        event_score = 0.0
        for event, weight in special_events.items():
            if event in content:
                count = content.count(event)
                event_score += weight * count

        # é™åˆ¶äº‹ä»¶å¾—åˆ†ä¸Šé™
        event_score = min(event_score, 0.2)  # æœ€å¤š0.2åˆ†

        # æ€»åˆ†
        total_score = length_score + keyword_score + event_score

        # å½’ä¸€åŒ–åˆ° 0.0-1.0
        return min(max(total_score, 0.0), 1.0)

    def search_by_time(
        self,
        time_query: str,
        k: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ—¶é—´æŸ¥è¯¢æ—¥è®°

        Args:
            time_query: æ—¶é—´æŸ¥è¯¢ï¼ˆå¦‚ï¼š"æ˜¨å¤©"ã€"ä¸¤å¤©å‰"ã€"ä¸Šå‘¨"ï¼‰
            k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æ—¥è®°åˆ—è¡¨
        """
        if not self.diary_file:
            return []

        # è§£ææ—¶é—´æŸ¥è¯¢
        target_date = self._parse_time_query(time_query)
        if not target_date:
            logger.warning(f"æ— æ³•è§£ææ—¶é—´æŸ¥è¯¢: {time_query}")
            return []

        try:
            diaries = self._get_diaries()

            # ç­›é€‰ç›®æ ‡æ—¥æœŸçš„æ—¥è®°
            results = []
            for diary in diaries:
                diary_date = datetime.fromisoformat(diary["timestamp"]).date()
                if diary_date == target_date:
                    results.append(diary)

            logger.debug(f"æ—¶é—´æ£€ç´¢ [{time_query}]: æ‰¾åˆ° {len(results)} æ¡æ—¥è®°")
            return results[:k]

        except Exception as e:
            logger.error(f"æ—¶é—´æ£€ç´¢å¤±è´¥: {e}")
            return []

    def search_by_content(
        self,
        query: str,
        k: int = 3,
        emotion: Optional[str] = None,
        topic: Optional[str] = None,
        min_importance: Optional[float] = None,
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å†…å®¹æœç´¢æ—¥è®°ï¼ˆè¯­ä¹‰åŒ¹é… + å…ƒæ•°æ®è¿‡æ»¤ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            emotion: æƒ…æ„Ÿè¿‡æ»¤ï¼ˆå¦‚ï¼šhappy, sadï¼‰
            topic: ä¸»é¢˜è¿‡æ»¤ï¼ˆå¦‚ï¼šwork, lifeï¼‰
            min_importance: æœ€å°é‡è¦æ€§ï¼ˆ0.0-1.0ï¼‰

        Returns:
            List[Dict]: æ—¥è®°åˆ—è¡¨
        """
        if self.vectorstore is None:
            return []

        try:
            # v2.30.29: æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤
            filter_dict = {}
            if emotion:
                filter_dict["emotion"] = emotion
            if topic:
                filter_dict["topic"] = topic

            # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤ï¼ˆv2.48.5: ä½¿ç”¨æµ·è±¡è¿ç®—ç¬¦ä¼˜åŒ–ï¼‰
            results = self.vectorstore.similarity_search_with_score(
                query, k=k * 3, filter=filter_dict or None
            )

            memories = []
            for doc, score in results:
                similarity = 1.0 - score

                # åº”ç”¨é˜ˆå€¼
                if (
                    settings.agent.is_check_memorys
                    and similarity < settings.agent.mem_thresholds
                ):
                    continue

                # v2.48.5: ä½¿ç”¨æµ·è±¡è¿ç®—ç¬¦ä¼˜åŒ–é‡è¦æ€§è¿‡æ»¤
                if min_importance is not None and (doc_importance := doc.metadata.get("importance", 0.0)) < min_importance:
                    continue

                memories.append({
                    "content": doc.page_content,
                    "similarity": similarity,
                    "metadata": doc.metadata,
                })

                if len(memories) >= k:
                    break

            logger.debug(
                f"å†…å®¹æ£€ç´¢: æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³æ—¥è®° "
                f"(æƒ…æ„Ÿ:{emotion}, ä¸»é¢˜:{topic}, æœ€å°é‡è¦æ€§:{min_importance})"
            )
            return memories

        except Exception as e:
            logger.error(f"å†…å®¹æ£€ç´¢å¤±è´¥: {e}")
            return []

    def search_by_emotion(
        self,
        emotion: str,
        k: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰æƒ…æ„Ÿæœç´¢æ—¥è®°ï¼ˆv2.30.29 æ–°å¢ï¼‰

        Args:
            emotion: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆhappy, sad, angry, anxious, excited, neutralï¼‰
            k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æ—¥è®°åˆ—è¡¨
        """
        if not self.diary_file:
            return []

        try:
            diaries = self._get_diaries()

            # ç­›é€‰æŒ‡å®šæƒ…æ„Ÿçš„æ—¥è®°
            results = [
                diary for diary in diaries
                if diary.get("emotion") == emotion
            ]

            # æŒ‰é‡è¦æ€§æ’åº
            results.sort(key=lambda x: x.get("importance", 0.0), reverse=True)

            logger.debug(f"æƒ…æ„Ÿæ£€ç´¢ [{emotion}]: æ‰¾åˆ° {len(results)} æ¡æ—¥è®°")
            return results[:k]

        except Exception as e:
            logger.error(f"æƒ…æ„Ÿæ£€ç´¢å¤±è´¥: {e}")
            return []

    def search_by_topic(
        self,
        topic: str,
        k: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰ä¸»é¢˜æœç´¢æ—¥è®°ï¼ˆv2.30.29 æ–°å¢ï¼‰

        Args:
            topic: ä¸»é¢˜æ ‡ç­¾ï¼ˆwork, life, study, entertainment, health, relationship, otherï¼‰
            k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æ—¥è®°åˆ—è¡¨
        """
        if not self.diary_file:
            return []

        try:
            diaries = self._get_diaries()

            # ç­›é€‰æŒ‡å®šä¸»é¢˜çš„æ—¥è®°
            results = [
                diary for diary in diaries
                if diary.get("topic") == topic
            ]

            # æŒ‰é‡è¦æ€§æ’åº
            results.sort(key=lambda x: x.get("importance", 0.0), reverse=True)

            logger.debug(f"ä¸»é¢˜æ£€ç´¢ [{topic}]: æ‰¾åˆ° {len(results)} æ¡æ—¥è®°")
            return results[:k]

        except Exception as e:
            logger.error(f"ä¸»é¢˜æ£€ç´¢å¤±è´¥: {e}")
            return []

    def get_emotion_stats(self) -> Dict[str, int]:
        """
        è·å–æƒ…æ„Ÿç»Ÿè®¡ï¼ˆv2.30.29 æ–°å¢ï¼‰

        Returns:
            Dict: æƒ…æ„Ÿç»Ÿè®¡ {emotion: count}
        """
        if not self.diary_file:
            return {}

        try:
            diaries = self._get_diaries()

            # ç»Ÿè®¡æ¯ç§æƒ…æ„Ÿçš„æ•°é‡
            emotion_counts = {}
            for diary in diaries:
                emotion = diary.get("emotion", "neutral")
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

            return emotion_counts

        except Exception as e:
            logger.error(f"è·å–æƒ…æ„Ÿç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    def get_topic_stats(self) -> Dict[str, int]:
        """
        è·å–ä¸»é¢˜ç»Ÿè®¡ï¼ˆv2.30.29 æ–°å¢ï¼‰

        Returns:
            Dict: ä¸»é¢˜ç»Ÿè®¡ {topic: count}
        """
        if not self.diary_file:
            return {}

        try:
            diaries = self._get_diaries()

            # ç»Ÿè®¡æ¯ä¸ªä¸»é¢˜çš„æ•°é‡
            topic_counts = {}
            for diary in diaries:
                topic = diary.get("topic", "other")
                topic_counts[topic] = topic_counts.get(topic, 0) + 1

            return topic_counts

        except Exception as e:
            logger.error(f"è·å–ä¸»é¢˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    @staticmethod
    def _parse_time_query(time_query: str) -> Optional[datetime.date]:
        """
        è§£ææ—¶é—´æŸ¥è¯¢

        Args:
            time_query: æ—¶é—´æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            datetime.date: ç›®æ ‡æ—¥æœŸ
        """
        today = datetime.now().date()

        # åŒ¹é…æ¨¡å¼
        patterns = {
            r"ä»Šå¤©|ä»Šæ—¥": 0,
            r"æ˜¨å¤©|æ˜¨æ—¥": 1,
            r"å‰å¤©": 2,
            r"(\d+)å¤©å‰": lambda m: int(m.group(1)),
            r"ä¸Šå‘¨|ä¸€å‘¨å‰": 7,
            r"(\d+)å‘¨å‰": lambda m: int(m.group(1)) * 7,
        }

        for pattern, days in patterns.items():
            match = re.search(pattern, time_query)
            if match:
                if callable(days):
                    days = days(match)
                return today - timedelta(days=days)

        return None


class LoreBook:
    """
    çŸ¥è¯†åº“ï¼ˆä¸–ç•Œä¹¦ï¼‰- v2.30.38 å¢å¼ºç‰ˆ

    ç”¨äºç»™å¤§æ¨¡å‹æ·»åŠ çŸ¥è¯†ï¼Œå¦‚ï¼šäººç‰©ã€ç‰©å“ã€äº‹ä»¶ç­‰
    å¼ºåŒ– AI çš„èƒ½åŠ›ï¼Œä¹Ÿå¯ç”¨äºå¼ºåŒ–è§’è‰²æ‰®æ¼”

    v2.30.38 æ–°å¢åŠŸèƒ½ï¼š
    - æ›´æ–°çŸ¥è¯†æ¡ç›®
    - åˆ é™¤çŸ¥è¯†æ¡ç›®
    - æ‰¹é‡å¯¼å…¥/å¯¼å‡º
    - ç»Ÿè®¡ä¿¡æ¯
    - æ™ºèƒ½å­¦ä¹ ï¼ˆä»å¯¹è¯ã€æ–‡ä»¶ã€MCPå­¦ä¹ ï¼‰
    """

    def __init__(self, persist_directory: Optional[str] = None, user_id: Optional[int] = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“

        Args:
            persist_directory: æŒä¹…åŒ–ç›®å½•
            user_id: ç”¨æˆ·IDï¼Œç”¨äºåˆ›å»ºç”¨æˆ·ç‰¹å®šçš„è®°å¿†è·¯å¾„
        """
        # å¹¶å‘è¯»å†™ä¿æŠ¤ï¼šè´¨é‡æ£€æŸ¥ç­‰åå°ä»»åŠ¡å¯èƒ½ä¸ä¸»çº¿ç¨‹åŒæ—¶è®¿é—® JSON/ç¼“å­˜
        self._lock = Lock()
        # ä½¿ç”¨æ¬¡æ•°å†™ç›˜ç¼“å†²ï¼ˆé¿å…æ¯æ¬¡ search éƒ½è¯»å†™ JSON + æ¸…ç¼“å­˜ï¼‰
        self._usage_buffer: Dict[str, int] = {}
        self._usage_pending_total: int = 0
        self._usage_flush_running: bool = False
        self._usage_last_flush: float = time.monotonic()
        self._usage_flush_interval_s: float = max(
            1.0, float(getattr(settings.agent, "lore_usage_flush_interval_s", 10.0))
        )
        self._usage_flush_max_pending: int = max(
            1, int(getattr(settings.agent, "lore_usage_flush_max_pending", 50))
        )

        if not settings.agent.lore_books:
            logger.info("çŸ¥è¯†åº“åŠŸèƒ½æœªå¯ç”¨")
            self.vectorstore = None
            self.json_file = None
            self._cache = None
            self.multi_cache = None
            self.async_processor = None
            return

        # æ”¯æŒç”¨æˆ·ç‰¹å®šè·¯å¾„
        if persist_directory:
            persist_dir = persist_directory
        elif user_id is not None:
            persist_dir = str(Path(settings.data_dir) / "users" / str(user_id) / "memory" / "lore_books")
        else:
            persist_dir = str(Path(settings.data_dir) / "memory" / "lore_books")

        Path(persist_dir).mkdir(parents=True, exist_ok=True)

        # v2.30.38: æ·»åŠ  JSON æ–‡ä»¶å­˜å‚¨ï¼ˆç”¨äºå…ƒæ•°æ®å’Œç®¡ç†ï¼‰
        self.json_file = Path(persist_dir) / "lore_books.json"
        if not self.json_file.exists():
            self.json_file.write_text("[]", encoding="utf-8")

        # ä½¿ç”¨ç»Ÿä¸€çš„ChromaDBåˆå§‹åŒ–å‡½æ•°ï¼ˆv2.30.27: æ”¯æŒæœ¬åœ° embedding å’Œç¼“å­˜ï¼‰
        self.vectorstore = create_chroma_vectorstore(
            collection_name="lore_books",
            persist_directory=persist_dir,
            use_local_embedding=settings.use_local_embedding,
            enable_cache=settings.enable_embedding_cache,
        )

        # v2.30.39: æ·»åŠ å†…å­˜ç¼“å­˜ï¼ˆæå‡æ€§èƒ½ï¼‰
        self._cache = {
            "all_lores": None,  # ç¼“å­˜æ‰€æœ‰çŸ¥è¯†
            "statistics": None,  # ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
            "last_update": None,  # æœ€åæ›´æ–°æ—¶é—´
        }

        # v2.30.40: åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        self.hybrid_retriever = None
        self.reranker = None
        self.query_expander = None

        if HAS_HYBRID_RETRIEVER:
            # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨ç¬¬ä¸€æ¬¡æœç´¢æ—¶æ„å»º
            self.reranker = Reranker()
            self.query_expander = QueryExpander()
            logger.info("æ··åˆæ£€ç´¢ç³»ç»Ÿå·²å¯ç”¨")

        # v2.30.41: åˆå§‹åŒ–çŸ¥è¯†è´¨é‡ç®¡ç†å™¨
        self.quality_manager = None
        if HAS_QUALITY_MANAGER:
            self.quality_manager = KnowledgeQualityManager()
            logger.info("çŸ¥è¯†è´¨é‡ç®¡ç†ç³»ç»Ÿå·²å¯ç”¨")

        # v2.30.42: åˆå§‹åŒ–çŸ¥è¯†æ¨èç³»ç»Ÿ
        self.recommender = None
        self.pusher = None
        self.usage_tracker = None

        if HAS_RECOMMENDER:
            self.recommender = KnowledgeRecommender()
            self.pusher = ProactiveKnowledgePusher()
            self.usage_tracker = KnowledgeUsageTracker()
            logger.info("çŸ¥è¯†æ¨èç³»ç»Ÿå·²å¯ç”¨")

        # v2.30.43: åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç³»ç»Ÿ
        self.knowledge_graph = None

        if HAS_KNOWLEDGE_GRAPH:
            self.knowledge_graph = KnowledgeGraph()
            logger.info("çŸ¥è¯†å›¾è°±ç³»ç»Ÿå·²å¯ç”¨")

        # v2.30.44: åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
        self.multi_cache = None
        self.async_processor = None

        if HAS_PERFORMANCE_OPTIMIZER:
            # å¤šçº§ç¼“å­˜
            self.multi_cache = MultiLevelCache(
                redis_host=getattr(settings.agent, "redis_host", "localhost"),
                redis_port=getattr(settings.agent, "redis_port", 6379),
                redis_db=getattr(settings.agent, "redis_db", 0),
                redis_password=getattr(settings.agent, "redis_password", None),
                default_ttl=3600,  # 1å°æ—¶
                max_memory_items=1000,
                enable_redis=getattr(settings.agent, "redis_enabled", True),
                connect_timeout=getattr(settings.agent, "redis_connect_timeout", 2.0),
                socket_timeout=getattr(settings.agent, "redis_socket_timeout", 2.0),
            )

            # å¼‚æ­¥å¤„ç†å™¨
            self.async_processor = AsyncProcessor(max_workers=4)

            logger.info("æ€§èƒ½ä¼˜åŒ–å™¨å·²å¯ç”¨ï¼ˆå¤šçº§ç¼“å­˜ + å¼‚æ­¥å¤„ç†ï¼‰")

        if self.vectorstore:
            count = get_collection_count(self.vectorstore)
            logger.info(f"çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼Œå·²æœ‰çŸ¥è¯†: {count} æ¡")

    def _perform_quality_check(
        self,
        title: str,
        content: str,
        category: str,
        keywords: Optional[List[str]],
        source: str,
    ) -> None:
        """
        æ‰§è¡ŒçŸ¥è¯†è´¨é‡æ£€æŸ¥ï¼ˆè¾…åŠ©æ–¹æ³•ï¼Œv2.48.4ï¼‰

        Args:
            title: çŸ¥è¯†æ ‡é¢˜
            content: çŸ¥è¯†å†…å®¹
            category: ç±»åˆ«
            keywords: å…³é”®è¯åˆ—è¡¨
            source: æ¥æº
        """
        if not self.quality_manager:
            return

        knowledge_data = {
            "title": title,
            "content": content,
            "category": category,
            "keywords": ",".join(keywords) if keywords else "",
            "source": source,
            "timestamp": datetime.now().isoformat(),
        }

        # è·å–ç°æœ‰çŸ¥è¯†ï¼ˆç”¨äºå†²çªæ£€æµ‹ï¼‰
        existing_knowledge = self.get_all_lores(use_cache=True)

        # è¯„ä¼°çŸ¥è¯†è´¨é‡
        assessment = self.quality_manager.assess_knowledge(
            knowledge_data, existing_knowledge
        )

        # è®°å½•è¯„ä¼°ç»“æœ
        if not assessment["is_valid"]:
            logger.warning(f"çŸ¥è¯†éªŒè¯å¤±è´¥: {assessment['issues']}")

        if assessment["has_conflicts"]:
            logger.warning(f"æ£€æµ‹åˆ°çŸ¥è¯†å†²çª: {len(assessment['conflicts'])} ä¸ª")

        if assessment["quality_score"] < 0.3:
            logger.warning(f"çŸ¥è¯†è´¨é‡è¾ƒä½: {assessment['quality_score']:.2f}")

        logger.info(f"çŸ¥è¯†è´¨é‡è¯„åˆ†: {assessment['quality_score']:.2f}")

    def _create_lore_metadata(
        self,
        lore_id: str,
        title: str,
        category: str,
        keywords: Optional[List[str]],
        source: str,
    ) -> Dict[str, Any]:
        """
        åˆ›å»ºçŸ¥è¯†å…ƒæ•°æ®ï¼ˆè¾…åŠ©æ–¹æ³•ï¼Œv2.48.4ï¼‰

        Args:
            lore_id: çŸ¥è¯†ID
            title: çŸ¥è¯†æ ‡é¢˜
            category: ç±»åˆ«
            keywords: å…³é”®è¯åˆ—è¡¨
            source: æ¥æº

        Returns:
            Dict[str, Any]: å…ƒæ•°æ®å­—å…¸
        """
        return {
            "id": lore_id,
            "title": title,
            "category": category,
            "keywords": ",".join(keywords) if keywords else "",
            "source": source,
            "timestamp": datetime.now().isoformat(),
            "update_count": 0,
            "usage_count": 0,
            "positive_feedback": 0,
            "negative_feedback": 0,
        }

    def add_lore(
        self,
        title: str,
        content: str,
        category: str = "general",
        keywords: Optional[List[str]] = None,
        source: str = "manual",  # v2.30.38: æ·»åŠ æ¥æºæ ‡è®°
        skip_quality_check: bool = False,  # v2.30.41: æ˜¯å¦è·³è¿‡è´¨é‡æ£€æŸ¥
    ) -> Optional[str]:
        """
        æ·»åŠ çŸ¥è¯†æ¡ç›® - v2.30.41 å¢å¼ºç‰ˆ

        v2.48.4 é‡æ„ä¼˜åŒ–ï¼š
        - æå–è¾…åŠ©æ–¹æ³•ï¼Œå‡å°‘å‡½æ•°é•¿åº¦
        - æé«˜ä»£ç å¯ç»´æŠ¤æ€§

        Args:
            title: çŸ¥è¯†æ ‡é¢˜
            content: çŸ¥è¯†å†…å®¹
            category: ç±»åˆ«ï¼ˆå¦‚ï¼šcharacter, item, event, location, generalï¼‰
            keywords: å…³é”®è¯åˆ—è¡¨
            source: æ¥æºï¼ˆmanual, conversation, file, mcpï¼‰
            skip_quality_check: æ˜¯å¦è·³è¿‡è´¨é‡æ£€æŸ¥

        Returns:
            str: çŸ¥è¯†IDï¼Œå¤±è´¥è¿”å› None
        """
        if self.vectorstore is None:
            return None

        # v2.48.4: ä½¿ç”¨è¾…åŠ©æ–¹æ³•è¿›è¡Œè´¨é‡æ£€æŸ¥
        if not skip_quality_check:
            # è´¨é‡æ£€æŸ¥åªäº§ç”Ÿå‘Šè­¦/è¯„åˆ†ï¼Œä¸å½±å“å†™å…¥ç»“æœï¼›æ”¾åˆ°åå°é¿å…é˜»å¡ä¸»æµç¨‹
            if self.async_processor:
                try:
                    self.async_processor.submit(
                        self._perform_quality_check,
                        title,
                        content,
                        category,
                        keywords,
                        source,
                    )
                except Exception as exc:
                    logger.debug("æäº¤è´¨é‡æ£€æŸ¥ä»»åŠ¡å¤±è´¥ï¼Œå›é€€ä¸ºåŒæ­¥æ‰§è¡Œ: %s", exc)
                    self._perform_quality_check(title, content, category, keywords, source)
            else:
                self._perform_quality_check(title, content, category, keywords, source)

        # v2.30.38: ç”Ÿæˆå”¯ä¸€ID
        import uuid
        lore_id = str(uuid.uuid4())

        # v2.48.4: ä½¿ç”¨è¾…åŠ©æ–¹æ³•åˆ›å»ºå…ƒæ•°æ®
        metadata = self._create_lore_metadata(lore_id, title, category, keywords, source)

        # ç»„åˆæ ‡é¢˜å’Œå†…å®¹
        full_content = f"ã€{title}ã€‘\n{content}"

        try:
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.vectorstore.add_texts(
                texts=[full_content],
                metadatas=[metadata],
                ids=[lore_id],
            )

            # v2.30.38: ä¿å­˜åˆ° JSON æ–‡ä»¶
            self._save_to_json({
                "id": lore_id,
                "title": title,
                "content": content,
                "category": category,
                "keywords": keywords or [],
                "source": source,
                "timestamp": metadata["timestamp"],
                "update_count": 0,
                "usage_count": 0,
                "positive_feedback": 0,
                "negative_feedback": 0,
            })

            # v2.30.39: æ¸…é™¤ç¼“å­˜
            self._invalidate_cache()

            logger.info(f"æ·»åŠ çŸ¥è¯† [{category}] [{source}]: {title}")
            return lore_id

        except Exception as e:
            logger.error(f"æ·»åŠ çŸ¥è¯†å¤±è´¥: {e}")
            return None

    def close(self) -> None:
        """æ˜¾å¼æ¸…ç†èµ„æºï¼ˆçº¿ç¨‹æ± ã€ç¼“å­˜ç­‰ï¼‰ã€‚"""
        try:
            # æœ€ç»ˆåˆ·ç›˜ï¼šæŠŠå°šæœªè½ç›˜çš„ usage_count å†™å› JSON
            self._flush_usage_counts()
            if self.async_processor:
                self.async_processor.close()
        except Exception as exc:
            logger.debug("å…³é—­å¼‚æ­¥å¤„ç†å™¨å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: %s", exc)
        finally:
            self.async_processor = None

    def update_lore(
        self,
        lore_id: str,
        title: Optional[str] = None,
        content: Optional[str] = None,
        category: Optional[str] = None,
        keywords: Optional[List[str]] = None,
    ) -> bool:
        """
        æ›´æ–°çŸ¥è¯†æ¡ç›® - v2.30.38 æ–°å¢

        Args:
            lore_id: çŸ¥è¯†ID
            title: æ–°æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            content: æ–°å†…å®¹ï¼ˆå¯é€‰ï¼‰
            category: æ–°ç±»åˆ«ï¼ˆå¯é€‰ï¼‰
            keywords: æ–°å…³é”®è¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.vectorstore is None:
            return False

        try:
            # ä» JSON æ–‡ä»¶è¯»å–åŸå§‹æ•°æ®
            lore_data = self._get_lore_by_id(lore_id)
            if not lore_data:
                logger.warning(f"çŸ¥è¯†IDä¸å­˜åœ¨: {lore_id}")
                return False

            # æ›´æ–°å­—æ®µ
            if title is not None:
                lore_data["title"] = title
            if content is not None:
                lore_data["content"] = content
            if category is not None:
                lore_data["category"] = category
            if keywords is not None:
                lore_data["keywords"] = keywords

            lore_data["update_count"] = lore_data.get("update_count", 0) + 1
            lore_data["last_update"] = datetime.now().isoformat()

            # åˆ é™¤æ—§çš„å‘é‡
            self.vectorstore.delete(ids=[lore_id])

            # æ·»åŠ æ–°çš„å‘é‡
            full_content = f"ã€{lore_data['title']}ã€‘\n{lore_data['content']}"
            metadata = {
                "id": lore_id,
                "title": lore_data["title"],
                "category": lore_data["category"],
                "keywords": ",".join(lore_data["keywords"]) if lore_data["keywords"] else "",
                "source": lore_data.get("source", "manual"),
                "timestamp": lore_data["timestamp"],
                "update_count": lore_data["update_count"],
                "last_update": lore_data["last_update"],
            }

            self.vectorstore.add_texts(
                texts=[full_content],
                metadatas=[metadata],
                ids=[lore_id],
            )

            # æ›´æ–° JSON æ–‡ä»¶
            self._update_json(lore_data)

            # v2.30.39: æ¸…é™¤ç¼“å­˜
            self._invalidate_cache()

            logger.info(f"æ›´æ–°çŸ¥è¯†: {lore_data['title']}")
            return True

        except Exception as e:
            logger.error(f"æ›´æ–°çŸ¥è¯†å¤±è´¥: {e}")
            return False

    def delete_lore(self, lore_id: str) -> bool:
        """
        åˆ é™¤çŸ¥è¯†æ¡ç›® - v2.30.38 æ–°å¢

        Args:
            lore_id: çŸ¥è¯†ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.vectorstore is None:
            return False

        try:
            # ä»å‘é‡æ•°æ®åº“åˆ é™¤
            self.vectorstore.delete(ids=[lore_id])

            # ä» JSON æ–‡ä»¶åˆ é™¤
            self._delete_from_json(lore_id)

            # v2.30.39: æ¸…é™¤ç¼“å­˜
            self._invalidate_cache()

            logger.info(f"åˆ é™¤çŸ¥è¯†: {lore_id}")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤çŸ¥è¯†å¤±è´¥: {e}")
            return False

    def _ensure_hybrid_retriever(self):
        """
        ç¡®ä¿æ··åˆæ£€ç´¢å™¨å·²åˆå§‹åŒ– (v2.30.40)

        å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ–‡æ¡£
        """
        if not HAS_HYBRID_RETRIEVER or self.hybrid_retriever is not None:
            return

        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_lores = self.get_all_lores(use_cache=True)

            if all_lores:
                # æ„å»ºæ··åˆæ£€ç´¢å™¨
                self.hybrid_retriever = HybridRetriever(
                    vectorstore=self.vectorstore,
                    documents=all_lores
                )
                logger.info(f"æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(all_lores)}")
        except Exception as e:
            logger.error(f"æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

    def _search_with_hybrid_retriever(
        self,
        query: str,
        k: int,
        category: Optional[str],
        use_rerank: bool,
        context: Optional[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨æ··åˆæ£€ç´¢å™¨æœç´¢ï¼ˆè¾…åŠ©æ–¹æ³•ï¼Œv2.48.4ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            category: ç­›é€‰ç±»åˆ«
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœ
        """
        # ç¡®ä¿æ··åˆæ£€ç´¢å™¨å·²åˆå§‹åŒ–
        self._ensure_hybrid_retriever()

        if not self.hybrid_retriever:
            return []

        # æ··åˆæ£€ç´¢
        results = self.hybrid_retriever.search(
            query=query,
            k=k,
            alpha=0.6,  # å‘é‡æ£€ç´¢æƒé‡ 60%ï¼ŒBM25 æƒé‡ 40%
            category=category,
            threshold=settings.agent.books_thresholds,
        )

        # é‡æ’åº
        if use_rerank and self.reranker and results:
            results = self.reranker.rerank(
                results=results,
                query=query,
                context=context or {},
            )

        # è½¬æ¢æ ¼å¼
        lores = []
        for result in results:
            lores.append({
                "content": result.get("content", ""),
                "similarity": result.get("final_score", result.get("score", 0.0)),
                "metadata": result.get("metadata", {}),
            })

        logger.debug(f"æ··åˆæ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(lores)} æ¡ç›¸å…³çŸ¥è¯†")
        return lores

    def _search_with_vector_store(
        self,
        query: str,
        k: int,
        category: Optional[str],
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ä¼ ç»Ÿå‘é‡æ£€ç´¢ï¼ˆè¾…åŠ©æ–¹æ³•ï¼Œv2.48.4ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            category: ç­›é€‰ç±»åˆ«

        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœ
        """
        results = self.vectorstore.similarity_search_with_score(query, k=k * 2)

        lores = []
        for doc, score in results:
            similarity = 1.0 - score

            # åº”ç”¨é˜ˆå€¼
            if similarity < settings.agent.books_thresholds:
                continue

            # ç±»åˆ«è¿‡æ»¤
            if category and doc.metadata.get("category") != category:
                continue

            lores.append({
                "content": doc.page_content,
                "similarity": similarity,
                "metadata": doc.metadata,
            })

            if len(lores) >= k:
                break

        logger.debug(f"å‘é‡æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(lores)} æ¡ç›¸å…³çŸ¥è¯†")
        return lores

    def search_lore(
        self,
        query: str,
        k: Optional[int] = None,
        category: Optional[str] = None,
        use_hybrid: bool = True,
        use_rerank: bool = True,
        context: Optional[Dict[str, Any]] = None,
        use_cache: bool = True,  # v2.30.44: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢çŸ¥è¯†åº“ (v2.30.44 å¢å¼ºç‰ˆ)

        v2.48.4 é‡æ„ä¼˜åŒ–ï¼š
        - æå–è¾…åŠ©æ–¹æ³•ï¼Œå‡å°‘å‡½æ•°é•¿åº¦
        - æé«˜ä»£ç å¯ç»´æŠ¤æ€§

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨ scan_depthï¼‰
            category: ç­›é€‰ç±»åˆ«
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆé»˜è®¤ Trueï¼‰
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åºï¼ˆé»˜è®¤ Trueï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºé‡æ’åºï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            List[Dict]: çŸ¥è¯†åˆ—è¡¨
        """
        if self.vectorstore is None:
            return []

        k = k or settings.agent.scan_depth

        # v2.30.44: å°è¯•ä»ç¼“å­˜è·å–
        # æ³¨æ„ï¼šquery å¯èƒ½å¾ˆé•¿ï¼Œç›´æ¥æ‹¼æ¥ä¼šå¯¼è‡´ç¼“å­˜ key è¿‡å¤§/å†…å­˜æ”¾å¤§ï¼›è¿™é‡Œä½¿ç”¨ hash å›ºå®šé•¿åº¦
        query_hash = hashlib.md5(str(query).encode("utf-8")).hexdigest()
        cache_key = f"search:{query_hash}:{k}:{category}:{use_hybrid}:{use_rerank}"
        if use_cache and self.multi_cache:
            cached_results = self.multi_cache.get(cache_key, prefix="lorebook")
            if cached_results is not None:
                logger.debug(
                    "ä»ç¼“å­˜è·å–æœç´¢ç»“æœ: query_hash=%s query=%.80s",
                    query_hash,
                    query,
                )
                return cached_results

        try:
            # v2.48.4: ä½¿ç”¨è¾…åŠ©æ–¹æ³•è¿›è¡Œæ£€ç´¢
            if use_hybrid and HAS_HYBRID_RETRIEVER:
                lores = self._search_with_hybrid_retriever(
                    query, k, category, use_rerank, context
                )
            else:
                lores = self._search_with_vector_store(query, k, category)

            # æ›´æ–°ä½¿ç”¨æ¬¡æ•°
            self._update_usage_count(lores)

            # v2.30.44: ä¿å­˜åˆ°ç¼“å­˜
            if use_cache and self.multi_cache:
                self.multi_cache.set(cache_key, lores, ttl=600, prefix="lorebook")

            return lores

        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}")
            return []

    def _update_usage_count(self, lores: List[Dict[str, Any]]):
        """
        æ›´æ–°çŸ¥è¯†ä½¿ç”¨æ¬¡æ•° (v2.30.40)

        Args:
            lores: çŸ¥è¯†åˆ—è¡¨
        """
        if not lores or self.json_file is None:
            return

        # ç»Ÿè®¡æœ¬æ¬¡å‘½ä¸­çš„çŸ¥è¯† IDï¼ˆé€šå¸¸ k å¾ˆå°ï¼Œé¿å… O(n^2) æ‰«æï¼‰
        increments: Dict[str, int] = {}
        for lore in lores:
            lore_id = None
            try:
                lore_id = lore.get("metadata", {}).get("id")  # vectorstore è¿”å›ç»“æ„
            except Exception:
                lore_id = None
            lore_id = lore_id or lore.get("id")
            if lore_id:
                lore_id = str(lore_id)
                increments[lore_id] = increments.get(lore_id, 0) + 1

        if not increments:
            return

        should_flush = False
        now = time.monotonic()
        with self._lock:
            for lore_id, delta in increments.items():
                self._usage_buffer[lore_id] = self._usage_buffer.get(lore_id, 0) + int(delta)
                self._usage_pending_total += int(delta)

            if (
                not self._usage_flush_running
                and self._usage_pending_total > 0
                and (
                    self._usage_pending_total >= self._usage_flush_max_pending
                    or (now - self._usage_last_flush) >= self._usage_flush_interval_s
                )
            ):
                self._usage_flush_running = True
                should_flush = True

        if not should_flush:
            return

        try:
            if self.async_processor:
                self.async_processor.submit(self._flush_usage_counts)
            else:
                self._flush_usage_counts()
        except Exception as e:
            with self._lock:
                self._usage_flush_running = False
            logger.warning(f"æäº¤ä½¿ç”¨æ¬¡æ•°åˆ·æ–°ä»»åŠ¡å¤±è´¥: {e}")

    def _flush_usage_counts(self) -> None:
        """
        å°†ç´¯ç§¯çš„ usage_count å†™å› JSONï¼ˆåå°/æ”¶å°¾ä½¿ç”¨ï¼‰ã€‚

        è®¾è®¡ç›®æ ‡ï¼š
        - ä¸åœ¨æ¯æ¬¡ search æ—¶å†™ç›˜
        - flush å¤±è´¥ä¸ä¸¢æ•°æ®ï¼ˆå›å¡«ç¼“å†²ï¼‰
        - ä¸ add/update/delete ç­‰å†™æ“ä½œå…±äº«åŒä¸€æŠŠé”ï¼Œé¿å…å¹¶å‘ JSON æŸå/ä¸¢æ›´æ–°
        """
        if self.json_file is None:
            with self._lock:
                self._usage_flush_running = False
            return

        pending: Dict[str, int] = {}
        with self._lock:
            if not self._usage_buffer:
                self._usage_flush_running = False
                return
            pending = dict(self._usage_buffer)
            self._usage_buffer.clear()
            self._usage_pending_total = 0

        try:
            updated = False
            with self._lock:
                records = self._read_json_records_unlocked()
                if not records:
                    self._usage_last_flush = time.monotonic()
                    return

                by_id = {str(r.get("id")): r for r in records if r.get("id")}
                for lore_id, delta in pending.items():
                    record = by_id.get(str(lore_id))
                    if record is None:
                        continue
                    try:
                        record["usage_count"] = int(record.get("usage_count", 0)) + int(delta)
                    except Exception:
                        record["usage_count"] = int(delta)
                    updated = True

                if updated:
                    self._write_json_records_unlocked(records)
                self._usage_last_flush = time.monotonic()

        except Exception as e:
            # å†™ç›˜å¤±è´¥ï¼šå›å¡«ç¼“å†²ï¼Œé¿å…ä¸¢å¤±
            with self._lock:
                for lore_id, delta in pending.items():
                    self._usage_buffer[lore_id] = self._usage_buffer.get(lore_id, 0) + int(delta)
                    self._usage_pending_total += int(delta)
            logger.warning(f"åˆ·æ–°ä½¿ç”¨æ¬¡æ•°å¤±è´¥: {e}")
        finally:
            with self._lock:
                self._usage_flush_running = False

    def get_all_lores(self, use_cache: bool = True) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰çŸ¥è¯†æ¡ç›® - v2.30.44 å¢å¼ºç‰ˆï¼ˆæ”¯æŒå¤šçº§ç¼“å­˜ï¼‰

        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            List[Dict]: æ‰€æœ‰çŸ¥è¯†åˆ—è¡¨
        """
        if self.json_file is None or not self.json_file.exists():
            return []

        # v2.30.44: å°è¯•ä»å¤šçº§ç¼“å­˜è·å–
        if use_cache and self.multi_cache:
            cached_data = self.multi_cache.get("all_lores", prefix="lorebook")
            if cached_data is not None:
                logger.debug("ä»å¤šçº§ç¼“å­˜è·å–æ‰€æœ‰çŸ¥è¯†")
                return cached_data

        # v2.30.39: ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        if use_cache and self._cache and self._cache["all_lores"] is not None:
            return self._cache["all_lores"]

        try:
            data = self._read_json_records()

            # v2.30.44: ä¿å­˜åˆ°å¤šçº§ç¼“å­˜
            if use_cache and self.multi_cache:
                self.multi_cache.set("all_lores", data, ttl=300, prefix="lorebook")  # 5åˆ†é’Ÿ

            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            if self._cache is not None:
                self._cache["all_lores"] = data
                self._cache["last_update"] = datetime.now()

            return data
        except Exception as e:
            logger.error(f"è¯»å–çŸ¥è¯†åº“å¤±è´¥: {e}")
            return []

    def get_lore_by_id(self, lore_id: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–çŸ¥è¯†æ¡ç›® - v2.30.38 æ–°å¢

        Args:
            lore_id: çŸ¥è¯†ID

        Returns:
            Dict: çŸ¥è¯†æ•°æ®ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        return self._get_lore_by_id(lore_id)

    def get_statistics(self, use_cache: bool = True) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯ - v2.30.44 å¢å¼ºç‰ˆï¼ˆæ”¯æŒå¤šçº§ç¼“å­˜ï¼‰

        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        # v2.30.44: å°è¯•ä»å¤šçº§ç¼“å­˜è·å–
        if use_cache and self.multi_cache:
            cached_stats = self.multi_cache.get("statistics", prefix="lorebook")
            if cached_stats is not None:
                logger.debug("ä»å¤šçº§ç¼“å­˜è·å–ç»Ÿè®¡ä¿¡æ¯")
                return cached_stats

        # v2.30.39: ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        if use_cache and self._cache and self._cache["statistics"] is not None:
            return self._cache["statistics"]

        all_lores = self.get_all_lores(use_cache=use_cache)

        if not all_lores:
            return {
                "total": 0,
                "by_category": {},
                "by_source": {},
                "recent_count": 0,
            }

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        by_category = {}
        for lore in all_lores:
            category = lore.get("category", "general")
            by_category[category] = by_category.get(category, 0) + 1

        # æŒ‰æ¥æºç»Ÿè®¡
        by_source = {}
        for lore in all_lores:
            source = lore.get("source", "manual")
            by_source[source] = by_source.get(source, 0) + 1

        # æœ€è¿‘7å¤©æ–°å¢æ•°é‡
        from datetime import timedelta
        seven_days_ago = datetime.now() - timedelta(days=7)
        recent_count = 0
        for lore in all_lores:
            timestamp_str = lore.get("timestamp", "")
            if timestamp_str:
                try:
                    timestamp = datetime.fromisoformat(timestamp_str)
                    if timestamp >= seven_days_ago:
                        recent_count += 1
                except (ValueError, TypeError) as e:
                    # å¿½ç•¥æ— æ•ˆçš„æ—¶é—´æˆ³æ ¼å¼
                    logger.debug(f"æ— æ•ˆçš„æ—¶é—´æˆ³æ ¼å¼: {timestamp_str}, é”™è¯¯: {e}")
                    pass

        stats = {
            "total": len(all_lores),
            "by_category": by_category,
            "by_source": by_source,
            "recent_count": recent_count,
        }

        # v2.30.44: ä¿å­˜åˆ°å¤šçº§ç¼“å­˜
        if use_cache and self.multi_cache:
            self.multi_cache.set("statistics", stats, ttl=300, prefix="lorebook")  # 5åˆ†é’Ÿ

        # æ›´æ–°æœ¬åœ°ç¼“å­˜
        if self._cache is not None:
            self._cache["statistics"] = stats

        return stats

    def export_to_json(self, filepath: str) -> bool:
        """
        å¯¼å‡ºçŸ¥è¯†åº“åˆ° JSON æ–‡ä»¶ - v2.30.38 æ–°å¢

        Args:
            filepath: å¯¼å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            all_lores = self.get_all_lores()
            Path(filepath).write_text(
                json.dumps(all_lores, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )
            logger.info(f"å¯¼å‡ºçŸ¥è¯†åº“æˆåŠŸ: {filepath} ({len(all_lores)} æ¡)")
            return True
        except Exception as e:
            logger.error(f"å¯¼å‡ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            return False

    def import_from_json(self, filepath: str, overwrite: bool = False) -> int:
        """
        ä» JSON æ–‡ä»¶å¯¼å…¥çŸ¥è¯†åº“ - v2.30.38 æ–°å¢

        Args:
            filepath: å¯¼å…¥æ–‡ä»¶è·¯å¾„
            overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„çŸ¥è¯†

        Returns:
            int: æˆåŠŸå¯¼å…¥çš„æ•°é‡
        """
        try:
            data = json.loads(Path(filepath).read_text(encoding="utf-8"))
            if not isinstance(data, list):
                logger.error("å¯¼å…¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šåº”è¯¥æ˜¯åˆ—è¡¨")
                return 0

            imported_count = 0
            for lore in data:
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                if "title" not in lore or "content" not in lore:
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ¡ç›®: {lore}")
                    continue

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                lore_id = lore.get("id")
                if lore_id and not overwrite:
                    existing = self._get_lore_by_id(lore_id)
                    if existing:
                        logger.debug(f"è·³è¿‡å·²å­˜åœ¨çš„çŸ¥è¯†: {lore['title']}")
                        continue

                # æ·»åŠ çŸ¥è¯†
                result = self.add_lore(
                    title=lore["title"],
                    content=lore["content"],
                    category=lore.get("category", "general"),
                    keywords=lore.get("keywords", []),
                    source=lore.get("source", "import"),
                )

                if result:
                    imported_count += 1

            logger.info(f"å¯¼å…¥çŸ¥è¯†åº“æˆåŠŸ: {filepath} ({imported_count} æ¡)")
            return imported_count

        except Exception as e:
            logger.error(f"å¯¼å…¥çŸ¥è¯†åº“å¤±è´¥: {e}")
            return 0

    def clear_all(self) -> bool:
        """
        æ¸…ç©ºæ‰€æœ‰çŸ¥è¯† - v2.30.38 æ–°å¢

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.json_file is None:
            return False

        previous_records: List[Dict[str, Any]] = []
        lore_ids: List[str] = []

        try:
            with self._lock:
                previous_records = self._read_json_records_unlocked()
                lore_ids = [str(l.get("id")) for l in previous_records if l.get("id")]
                self._write_json_records_unlocked([])

            if self.vectorstore and lore_ids:
                self.vectorstore.delete(ids=lore_ids)

            self._invalidate_cache()
            logger.info("æ¸…ç©ºçŸ¥è¯†åº“æˆåŠŸ")
            return True

        except Exception as e:
            # å°è¯•å›æ»š JSONï¼Œé¿å…â€œJSON å·²æ¸…ç©ºä½†å‘é‡åº“æœªæ¸…ç©ºâ€çš„ä¸ä¸€è‡´çŠ¶æ€
            try:
                with self._lock:
                    self._write_json_records_unlocked(previous_records)
            except Exception:
                pass

            logger.error(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            return False

    # ==================== ç§æœ‰è¾…åŠ©æ–¹æ³• ====================

    def _read_json_records_unlocked(self) -> List[Dict[str, Any]]:
        """è¯»å–çŸ¥è¯† JSONï¼ˆéœ€åœ¨ self._lock å†…è°ƒç”¨ï¼‰ã€‚"""
        if self.json_file is None or not self.json_file.exists():
            return []

        raw = ""
        try:
            raw = self.json_file.read_text(encoding="utf-8")
        except Exception as e:
            logger.error(f"è¯»å–çŸ¥è¯† JSON å¤±è´¥: {e}")
            return []

        if not raw.strip():
            return []

        try:
            data = json.loads(raw)
            return data if isinstance(data, list) else []
        except Exception as e:
            logger.error(f"è§£æçŸ¥è¯† JSON å¤±è´¥: {e}")
            return []

    def _write_json_records_unlocked(self, data: List[Dict[str, Any]]) -> None:
        """å†™å…¥çŸ¥è¯† JSONï¼ˆéœ€åœ¨ self._lock å†…è°ƒç”¨ï¼ŒåŸå­æ›¿æ¢é¿å…æŸåï¼‰ã€‚"""
        if self.json_file is None:
            return

        payload = json.dumps(data, ensure_ascii=False, indent=2)
        target = self.json_file
        temp_file = target.with_suffix(".tmp")

        try:
            temp_file.write_text(payload, encoding="utf-8")
            temp_file.replace(target)
        except Exception as e:
            logger.error(f"å†™å…¥çŸ¥è¯† JSON å¤±è´¥: {e}")
            try:
                if temp_file.exists():
                    temp_file.unlink(missing_ok=True)
            except Exception:
                pass

    def _read_json_records(self) -> List[Dict[str, Any]]:
        """çº¿ç¨‹å®‰å…¨åœ°è¯»å–çŸ¥è¯† JSON æ–‡ä»¶ï¼ˆè¿”å› listï¼‰ã€‚"""
        if self.json_file is None or not self.json_file.exists():
            return []
        with self._lock:
            return self._read_json_records_unlocked()

    def _write_json_records(self, data: List[Dict[str, Any]]) -> None:
        """çº¿ç¨‹å®‰å…¨åœ°å†™å…¥çŸ¥è¯† JSON æ–‡ä»¶ï¼ˆåŸå­æ›¿æ¢ï¼Œé¿å…å†™å…¥è¿‡ç¨‹ä¸­æŸåï¼‰ã€‚"""
        if self.json_file is None:
            return
        with self._lock:
            self._write_json_records_unlocked(data)

    def _save_to_json(self, lore_data: Dict[str, Any]) -> None:
        """ä¿å­˜çŸ¥è¯†åˆ° JSON æ–‡ä»¶"""
        if self.json_file is None:
            return

        try:
            with self._lock:
                data = self._read_json_records_unlocked()
                data.append(lore_data)
                self._write_json_records_unlocked(data)

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ° JSON å¤±è´¥: {e}")

    def _update_json(self, lore_data: Dict[str, Any]) -> None:
        """æ›´æ–° JSON æ–‡ä»¶ä¸­çš„çŸ¥è¯†"""
        if self.json_file is None:
            return

        try:
            with self._lock:
                data = self._read_json_records_unlocked()

                # æŸ¥æ‰¾å¹¶æ›´æ–°
                for i, lore in enumerate(data):
                    if lore.get("id") == lore_data.get("id"):
                        data[i] = lore_data
                        break

                self._write_json_records_unlocked(data)

        except Exception as e:
            logger.error(f"æ›´æ–° JSON å¤±è´¥: {e}")

    def _delete_from_json(self, lore_id: str) -> bool:
        """ä» JSON æ–‡ä»¶åˆ é™¤çŸ¥è¯†"""
        if self.json_file is None:
            return False

        try:
            with self._lock:
                data = self._read_json_records_unlocked()
                before = len(data)

                # è¿‡æ»¤æ‰è¦åˆ é™¤çš„æ¡ç›®
                data = [lore for lore in data if lore.get("id") != lore_id]
                deleted = len(data) != before

                if deleted:
                    self._write_json_records_unlocked(data)
                return deleted

        except Exception as e:
            logger.error(f"ä» JSON åˆ é™¤å¤±è´¥: {e}")
            return False

    def _get_lore_by_id(self, lore_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–çŸ¥è¯†"""
        if self.json_file is None or not self.json_file.exists():
            return None

        try:
            for lore in self.get_all_lores(use_cache=True):
                if lore.get("id") == lore_id:
                    return lore
            return None

        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†å¤±è´¥: {e}")
            return None

    # ==================== æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ (v2.30.38 æ–°å¢) ====================

    def learn_from_conversation(
        self,
        user_message: str,
        ai_response: str,
        auto_extract: bool = True,
        use_llm: bool = True,
    ) -> List[str]:
        """
        ä»å¯¹è¯ä¸­å­¦ä¹ çŸ¥è¯† - v2.30.39 å¢å¼ºç‰ˆï¼ˆæ”¯æŒ LLM è¾…åŠ©æå–ï¼‰

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            ai_response: AIå›å¤
            auto_extract: æ˜¯å¦è‡ªåŠ¨æå–çŸ¥è¯†
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM è¾…åŠ©æå–ï¼ˆæ›´æ™ºèƒ½ä½†æ›´æ…¢ï¼‰

        Returns:
            List[str]: å­¦ä¹ åˆ°çš„çŸ¥è¯†IDåˆ—è¡¨
        """
        if not auto_extract:
            return []

        learned_ids = []

        try:
            # v2.30.39: ä½¿ç”¨ LLM è¾…åŠ©æå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_llm and getattr(settings.agent, "use_llm_for_knowledge_extraction", False):
                learned_ids = self._llm_extract_knowledge(user_message, ai_response)
                if learned_ids:
                    return learned_ids

            # å›é€€åˆ°åŸºäºè§„åˆ™çš„æå–
            # æ£€æµ‹æ˜¯å¦åŒ…å«çŸ¥è¯†æ€§å†…å®¹
            knowledge_keywords = [
                "æ˜¯", "å«", "åå­—", "ä»‹ç»", "è¯´æ˜", "è§£é‡Š",
                "å®šä¹‰", "å«ä¹‰", "æ„æ€", "ç‰¹ç‚¹", "ç‰¹å¾",
                "ä½äº", "åœ¨", "åœ°ç‚¹", "åœ°æ–¹", "ä½ç½®",
                "ç”¨äº", "ç”¨æ¥", "ä½œç”¨", "åŠŸèƒ½", "ç”¨é€”",
                "å–œæ¬¢", "è®¨åŒ", "çˆ±å¥½", "å…´è¶£", "ä¹ æƒ¯",  # v2.30.39: æ–°å¢æƒ…æ„Ÿç›¸å…³
                "ç”Ÿæ—¥", "å¹´é¾„", "èº«é«˜", "ä½“é‡", "å¤–è²Œ",  # v2.30.39: æ–°å¢å±æ€§ç›¸å…³
            ]

            combined_text = user_message + " " + ai_response

            # ç®€å•çš„çŸ¥è¯†æå–é€»è¾‘
            if any(keyword in combined_text for keyword in knowledge_keywords):
                # æå–æ ‡é¢˜ï¼ˆä»ç”¨æˆ·æ¶ˆæ¯ä¸­ï¼‰
                title = self._extract_title_from_message(user_message)
                if not title:
                    return []

                # æå–å†…å®¹ï¼ˆä»AIå›å¤ä¸­ï¼‰
                content = ai_response

                # æå–ç±»åˆ«
                category = self._extract_category_from_content(combined_text)

                # æå–å…³é”®è¯
                keywords = self._extract_keywords_from_content(combined_text)

                # v2.30.39: æ£€æŸ¥æ˜¯å¦é‡å¤
                if self._is_duplicate_knowledge(title, content):
                    logger.debug(f"è·³è¿‡é‡å¤çŸ¥è¯†: {title}")
                    return []

                # æ·»åŠ çŸ¥è¯†
                lore_id = self.add_lore(
                    title=title,
                    content=content,
                    category=category,
                    keywords=keywords,
                    source="conversation",
                )

                if lore_id:
                    learned_ids.append(lore_id)
                    logger.info(f"ä»å¯¹è¯ä¸­å­¦ä¹ åˆ°çŸ¥è¯†: {title}")

        except Exception as e:
            logger.error(f"ä»å¯¹è¯ä¸­å­¦ä¹ å¤±è´¥: {e}")

        return learned_ids

    def learn_from_file(
        self,
        filepath: str,
        file_type: Optional[str] = None,
        chunk_size: int = 1000,
    ) -> List[str]:
        """
        ä»æ–‡ä»¶ä¸­å­¦ä¹ çŸ¥è¯† - v2.30.38 æ–°å¢

        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹ï¼ˆtxt, md, pdf, docxï¼‰
            chunk_size: åˆ†å—å¤§å°

        Returns:
            List[str]: å­¦ä¹ åˆ°çš„çŸ¥è¯†IDåˆ—è¡¨
        """
        learned_ids = []

        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            content = self._read_file_content(filepath, file_type)
            if not content:
                return []

            # åˆ†å—å¤„ç†
            chunks = self._split_content_into_chunks(content, chunk_size)

            # ä»æ¯ä¸ªå—ä¸­æå–çŸ¥è¯†
            for i, chunk in enumerate(chunks):
                # æå–æ ‡é¢˜
                title = self._extract_title_from_chunk(chunk, i)

                # æå–ç±»åˆ«
                category = self._extract_category_from_content(chunk)

                # æå–å…³é”®è¯
                keywords = self._extract_keywords_from_content(chunk)

                # æ·»åŠ çŸ¥è¯†
                lore_id = self.add_lore(
                    title=title,
                    content=chunk,
                    category=category,
                    keywords=keywords,
                    source="file",
                )

                if lore_id:
                    learned_ids.append(lore_id)

            logger.info(f"ä»æ–‡ä»¶ä¸­å­¦ä¹ åˆ° {len(learned_ids)} æ¡çŸ¥è¯†: {filepath}")

        except Exception as e:
            logger.error(f"ä»æ–‡ä»¶ä¸­å­¦ä¹ å¤±è´¥: {e}")

        return learned_ids

    def learn_from_mcp(
        self,
        mcp_data: Dict[str, Any],
        source_name: str = "mcp",
    ) -> Optional[str]:
        """
        ä» MCP æ•°æ®ä¸­å­¦ä¹ çŸ¥è¯† - v2.30.38 æ–°å¢

        Args:
            mcp_data: MCP è¿”å›çš„æ•°æ®
            source_name: MCP æ¥æºåç§°

        Returns:
            str: å­¦ä¹ åˆ°çš„çŸ¥è¯†IDï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # æå–æ ‡é¢˜
            title = mcp_data.get("title") or mcp_data.get("name") or "MCPæ•°æ®"

            # æå–å†…å®¹
            content = mcp_data.get("content") or mcp_data.get("description") or str(mcp_data)

            # æå–ç±»åˆ«
            category = mcp_data.get("category") or "general"

            # æå–å…³é”®è¯
            keywords = mcp_data.get("keywords") or []

            # æ·»åŠ çŸ¥è¯†
            lore_id = self.add_lore(
                title=title,
                content=content,
                category=category,
                keywords=keywords,
                source=f"mcp:{source_name}",
            )

            if lore_id:
                logger.info(f"ä» MCP ä¸­å­¦ä¹ åˆ°çŸ¥è¯†: {title}")

            return lore_id

        except Exception as e:
            logger.error(f"ä» MCP ä¸­å­¦ä¹ å¤±è´¥: {e}")
            return None

    # ==================== LLM è¾…åŠ©æå–æ–¹æ³• (v2.30.39 æ–°å¢) ====================

    def _llm_extract_knowledge(self, user_message: str, ai_response: str) -> List[str]:
        """
        ä½¿ç”¨ LLM è¾…åŠ©æå–çŸ¥è¯† - v2.30.39 æ–°å¢

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            ai_response: AIå›å¤

        Returns:
            List[str]: å­¦ä¹ åˆ°çš„çŸ¥è¯†IDåˆ—è¡¨
        """
        learned_ids = []

        try:
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import JsonOutputParser

            # æ„å»ºæç¤ºè¯
            extraction_prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æå–ä¸“å®¶ã€‚è¯·ä»å¯¹è¯ä¸­æå–æœ‰ä»·å€¼çš„çŸ¥è¯†ä¿¡æ¯ã€‚

æå–è§„åˆ™ï¼š
1. åªæå–äº‹å®æ€§ã€å¯è®°å½•çš„çŸ¥è¯†ï¼ˆäººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶ã€ç‰©å“ã€ç‰¹å¾ç­‰ï¼‰
2. å¿½ç•¥æ—¥å¸¸é—²èŠã€å¤©æ°”ã€å¿ƒæƒ…ç­‰ä¸´æ—¶æ€§ä¿¡æ¯
3. æ¯æ¡çŸ¥è¯†åº”è¯¥ç‹¬ç«‹ã€å®Œæ•´ã€æœ‰æ„ä¹‰

è¯·ä»¥ JSON æ ¼å¼è¿”å›æå–çš„çŸ¥è¯†åˆ—è¡¨ï¼š
{{
    "has_knowledge": true/false,
    "knowledge_list": [
        {{
            "title": "çŸ¥è¯†æ ‡é¢˜ï¼ˆç®€çŸ­æ¦‚æ‹¬ï¼‰",
            "content": "çŸ¥è¯†å†…å®¹ï¼ˆè¯¦ç»†æè¿°ï¼‰",
            "category": "ç±»åˆ«ï¼ˆcharacter/location/item/event/generalï¼‰",
            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...]
        }}
    ]
}}

å¦‚æœå¯¹è¯ä¸­æ²¡æœ‰å€¼å¾—è®°å½•çš„çŸ¥è¯†ï¼Œè¿”å› {{"has_knowledge": false, "knowledge_list": []}}"""),
                ("human", "ç”¨æˆ·: {user_message}\nAI: {ai_response}"),
            ])

            # åˆ›å»ºé“¾
            from src.llm.factory import get_llm
            llm = get_llm()
            parser = JsonOutputParser()
            chain = extraction_prompt | llm | parser

            # æ‰§è¡Œæå–
            result = chain.invoke({
                "user_message": user_message,
                "ai_response": ai_response,
            })

            # å¤„ç†ç»“æœ
            if result.get("has_knowledge") and result.get("knowledge_list"):
                for knowledge in result["knowledge_list"]:
                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                    if not knowledge.get("title") or not knowledge.get("content"):
                        continue

                    # æ£€æŸ¥æ˜¯å¦é‡å¤
                    if self._is_duplicate_knowledge(knowledge["title"], knowledge["content"]):
                        logger.debug(f"è·³è¿‡é‡å¤çŸ¥è¯†: {knowledge['title']}")
                        continue

                    # æ·»åŠ çŸ¥è¯†
                    lore_id = self.add_lore(
                        title=knowledge["title"],
                        content=knowledge["content"],
                        category=knowledge.get("category", "general"),
                        keywords=knowledge.get("keywords", []),
                        source="conversation:llm",
                    )

                    if lore_id:
                        learned_ids.append(lore_id)
                        logger.info(f"LLM æå–åˆ°çŸ¥è¯†: {knowledge['title']}")

        except Exception as e:
            logger.warning(f"LLM è¾…åŠ©æå–å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™æå–: {e}")

        return learned_ids

    def _is_duplicate_knowledge(self, title: str, content: str, threshold: float = 0.85) -> bool:
        """
        æ£€æŸ¥çŸ¥è¯†æ˜¯å¦é‡å¤ - v2.30.39 æ–°å¢

        Args:
            title: çŸ¥è¯†æ ‡é¢˜
            content: çŸ¥è¯†å†…å®¹
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.85ï¼‰

        Returns:
            bool: æ˜¯å¦é‡å¤
        """
        if self.vectorstore is None:
            return False

        try:
            # æœç´¢ç›¸ä¼¼çŸ¥è¯†
            query = f"{title} {content}"
            results = self.vectorstore.similarity_search_with_score(query, k=3)

            for doc, score in results:
                similarity = 1.0 - score

                # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                if similarity >= threshold:
                    logger.debug(f"å‘ç°ç›¸ä¼¼çŸ¥è¯†: {doc.metadata.get('title')} (ç›¸ä¼¼åº¦: {similarity:.2f})")
                    return True

            return False

        except Exception as e:
            logger.error(f"æ£€æŸ¥é‡å¤çŸ¥è¯†å¤±è´¥: {e}")
            return False

    # ==================== æ™ºèƒ½æå–è¾…åŠ©æ–¹æ³• ====================

    def _extract_title_from_message(self, message: str) -> Optional[str]:
        """ä»æ¶ˆæ¯ä¸­æå–æ ‡é¢˜"""
        # ç®€å•çš„æ ‡é¢˜æå–é€»è¾‘
        # æå–ç¬¬ä¸€å¥è¯æˆ–å‰20ä¸ªå­—ç¬¦
        lines = message.strip().split("\n")
        first_line = lines[0] if lines else message

        # ç§»é™¤é—®å·ç­‰
        title = first_line.replace("ï¼Ÿ", "").replace("?", "").strip()

        # é™åˆ¶é•¿åº¦
        if len(title) > 50:
            title = title[:50] + "..."

        return title if title else None

    def _extract_title_from_chunk(self, chunk: str, index: int) -> str:
        """ä»æ–‡æœ¬å—ä¸­æå–æ ‡é¢˜"""
        # å°è¯•æå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
        lines = chunk.strip().split("\n")
        first_line = lines[0] if lines else ""

        # å¦‚æœç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜æ ¼å¼ï¼ˆ# å¼€å¤´æˆ–å¾ˆçŸ­ï¼‰
        if first_line.startswith("#"):
            title = first_line.lstrip("#").strip()
        elif len(first_line) < 50:
            title = first_line.strip()
        else:
            # ä½¿ç”¨å‰30ä¸ªå­—ç¬¦
            title = chunk[:30].strip() + "..."

        # æ·»åŠ ç´¢å¼•
        if not title:
            title = f"çŸ¥è¯†ç‰‡æ®µ {index + 1}"

        return title

    def _extract_category_from_content(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–ç±»åˆ«"""
        # ç®€å•çš„ç±»åˆ«è¯†åˆ«
        for category, keywords in _CONTENT_CATEGORY_KEYWORDS.items():
            if any(keyword in content for keyword in keywords):
                return category

        return "general"

    def _extract_keywords_from_content(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆæå–åè¯ï¼‰
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„è§„åˆ™ï¼Œå®é™…å¯ä»¥ä½¿ç”¨ NLP å·¥å…·
        # æå–ä¸­æ–‡è¯è¯­ï¼ˆ2-4ä¸ªå­—ï¼‰
        words = _CHINESE_KEYWORDS_PATTERN.findall(content)

        # å»é‡å¹¶é™åˆ¶æ•°é‡ï¼ˆä¿æŒç¨³å®šé¡ºåºï¼Œé¿å… set å¸¦æ¥çš„éšæœºæ€§ï¼‰
        unique_words = list(dict.fromkeys(words))
        return unique_words[:10]

    def _read_file_content(self, filepath: str, file_type: Optional[str] = None) -> Optional[str]:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            path = Path(filepath)
            if not path.exists():
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                return None

            # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹
            if file_type is None:
                file_type = path.suffix.lower().lstrip(".")

            # è¯»å–ä¸åŒç±»å‹çš„æ–‡ä»¶
            if file_type in ["txt", "md"]:
                return path.read_text(encoding="utf-8")

            elif file_type == "pdf":
                # éœ€è¦ PyPDF2 æˆ– pdfplumber
                try:
                    import PyPDF2
                    with open(path, "rb") as f:
                        reader = PyPDF2.PdfReader(f)
                        text = ""
                        for page in reader.pages:
                            text += page.extract_text()
                        return text
                except ImportError:
                    logger.warning("éœ€è¦å®‰è£… PyPDF2: pip install PyPDF2")
                    return None

            elif file_type == "docx":
                # éœ€è¦ python-docx
                try:
                    import docx
                    doc = docx.Document(path)
                    text = "\n".join([para.text for para in doc.paragraphs])
                    return text
                except ImportError:
                    logger.warning("éœ€è¦å®‰è£… python-docx: pip install python-docx")
                    return None

            elif file_type in ["html", "htm"]:
                # v2.30.39: æ”¯æŒ HTML æ–‡ä»¶
                try:
                    from bs4 import BeautifulSoup
                    html_content = path.read_text(encoding="utf-8")
                    soup = BeautifulSoup(html_content, "html.parser")
                    # ç§»é™¤ script å’Œ style æ ‡ç­¾
                    for script in soup(["script", "style"]):
                        script.decompose()
                    text = soup.get_text(separator="\n\n")
                    # æ¸…ç†å¤šä½™ç©ºè¡Œ
                    lines = [line.strip() for line in text.split("\n") if line.strip()]
                    return "\n\n".join(lines)
                except ImportError:
                    logger.warning("éœ€è¦å®‰è£… beautifulsoup4: pip install beautifulsoup4")
                    return None

            elif file_type == "json":
                # v2.30.39: æ”¯æŒ JSON æ–‡ä»¶
                try:
                    data = json.loads(path.read_text(encoding="utf-8"))
                    # å°† JSON è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬
                    return json.dumps(data, ensure_ascii=False, indent=2)
                except Exception as e:
                    logger.error(f"è§£æ JSON æ–‡ä»¶å¤±è´¥: {e}")
                    return None

            elif file_type == "csv":
                # v2.30.39: æ”¯æŒ CSV æ–‡ä»¶
                try:
                    import csv
                    text_lines = []
                    with open(path, "r", encoding="utf-8") as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            # å°†æ¯è¡Œè½¬æ¢ä¸ºæ–‡æœ¬
                            line = ", ".join([f"{k}: {v}" for k, v in row.items()])
                            text_lines.append(line)
                    return "\n\n".join(text_lines)
                except Exception as e:
                    logger.error(f"è§£æ CSV æ–‡ä»¶å¤±è´¥: {e}")
                    return None

            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
                return None

        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def _split_content_into_chunks(
        self,
        content: str,
        chunk_size: int,
        overlap: int = 100,
    ) -> List[str]:
        """
        å°†å†…å®¹åˆ†å— - v2.30.39 å¢å¼ºç‰ˆï¼ˆæ”¯æŒé‡å ï¼‰

        Args:
            content: å†…å®¹
            chunk_size: åˆ†å—å¤§å°
            overlap: é‡å å¤§å°ï¼ˆé»˜è®¤ 100 å­—ç¬¦ï¼‰

        Returns:
            List[str]: åˆ†å—åˆ—è¡¨
        """
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split("\n\n")

        chunks = []
        current_chunk = ""
        previous_chunk_end = ""  # ç”¨äºé‡å 

        for para in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡å¤§å°
            if len(current_chunk) + len(para) < chunk_size:
                current_chunk += para + "\n\n"
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    # ä¿å­˜æœ«å°¾ç”¨äºé‡å 
                    previous_chunk_end = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk

                # å¼€å§‹æ–°å—ï¼ŒåŒ…å«é‡å éƒ¨åˆ†
                if previous_chunk_end:
                    current_chunk = previous_chunk_end + para + "\n\n"
                else:
                    current_chunk = para + "\n\n"

        # ä¿å­˜æœ€åä¸€å—
        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    # ==================== æ€§èƒ½ä¼˜åŒ–æ–¹æ³• (v2.30.39 æ–°å¢) ====================

    def _invalidate_cache(self, *, reset_hybrid: bool = True, clear_search_cache: bool = True) -> None:
        """æ¸…é™¤ç¼“å­˜ - v2.30.44 å¢å¼ºç‰ˆï¼ˆæ”¯æŒæ›´ç»†ç²’åº¦çš„å¤±æ•ˆç­–ç•¥ï¼‰ã€‚"""
        if self._cache is not None:
            self._cache["all_lores"] = None
            self._cache["statistics"] = None
            self._cache["last_update"] = datetime.now()
            logger.debug("çŸ¥è¯†åº“ç¼“å­˜å·²æ¸…é™¤")

        # v2.30.40: é‡ç½®æ··åˆæ£€ç´¢å™¨ï¼ˆéœ€è¦é‡æ–°æ„å»ºç´¢å¼•ï¼‰
        if reset_hybrid and HAS_HYBRID_RETRIEVER:
            self.hybrid_retriever = None
            logger.debug("æ··åˆæ£€ç´¢å™¨å·²é‡ç½®")

        # v2.30.44: æ¸…é™¤å¤šçº§ç¼“å­˜
        if self.multi_cache:
            if clear_search_cache:
                self.multi_cache.clear(prefix="lorebook")
                logger.debug("å¤šçº§ç¼“å­˜å·²æ¸…é™¤")
            else:
                # ä»…æ¸…ç†å…ƒæ•°æ®ç›¸å…³ç¼“å­˜ï¼Œä¿ç•™ search:* ç»“æœï¼Œé¿å…æ— è°“çš„ç¼“å­˜å‡»ç©¿
                self.multi_cache.delete("all_lores", prefix="lorebook")
                self.multi_cache.delete("statistics", prefix="lorebook")
                logger.debug("å¤šçº§ç¼“å­˜ï¼ˆå…ƒæ•°æ®ï¼‰å·²æ¸…é™¤")

    def batch_add_lores(self, lores: List[Dict[str, Any]]) -> List[str]:
        """
        æ‰¹é‡æ·»åŠ çŸ¥è¯† - v2.30.39 æ–°å¢

        Args:
            lores: çŸ¥è¯†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« title, content, category, keywords, source

        Returns:
            List[str]: æˆåŠŸæ·»åŠ çš„çŸ¥è¯†IDåˆ—è¡¨
        """
        if self.vectorstore is None:
            return []

        added_ids: List[str] = []

        try:
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            texts = []
            metadatas = []
            ids = []
            json_records: List[Dict[str, Any]] = []

            import uuid
            for lore in lores:
                # ç”ŸæˆID
                lore_id = str(uuid.uuid4())

                # å‡†å¤‡æ•°æ®
                title = lore.get("title", "")
                content = lore.get("content", "")
                category = lore.get("category", "general")
                keywords = lore.get("keywords", [])
                source = lore.get("source", "manual")

                full_content = f"ã€{title}ã€‘\n{content}"
                metadata = {
                    "id": lore_id,
                    "title": title,
                    "category": category,
                    "keywords": ",".join(keywords) if keywords else "",
                    "source": source,
                    "timestamp": datetime.now().isoformat(),
                    "update_count": 0,
                }

                texts.append(full_content)
                metadatas.append(metadata)
                ids.append(lore_id)
                json_records.append(
                    {
                        "id": lore_id,
                        "title": title,
                        "content": content,
                        "category": category,
                        "keywords": keywords or [],
                        "source": source,
                        "timestamp": metadata["timestamp"],
                        "update_count": 0,
                    }
                )

            # æ‰¹é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.vectorstore.add_texts(
                texts=texts,
                metadatas=metadatas,
                ids=ids,
            )

            try:
                with self._lock:
                    data = self._read_json_records_unlocked()
                    data.extend(json_records)
                    self._write_json_records_unlocked(data)
            except Exception as e:
                # å›æ»šå‘é‡åº“å†™å…¥ï¼Œé¿å…â€œå‘é‡åº“æœ‰ä½† JSON æ²¡æœ‰â€çš„ä¸ä¸€è‡´çŠ¶æ€
                logger.error(f"æ‰¹é‡ä¿å­˜åˆ° JSON å¤±è´¥ï¼Œå°†å›æ»šå‘é‡åº“å†™å…¥: {e}")
                try:
                    self.vectorstore.delete(ids=ids)
                except Exception as rollback_exc:
                    logger.warning(f"å›æ»šå‘é‡åº“å†™å…¥å¤±è´¥ï¼ˆå¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ï¼‰: {rollback_exc}")
                return []

            added_ids.extend(ids)

            # æ¸…é™¤ç¼“å­˜
            self._invalidate_cache()

            logger.info(f"æ‰¹é‡æ·»åŠ  {len(added_ids)} æ¡çŸ¥è¯†")
            return added_ids

        except Exception as e:
            logger.error(f"æ‰¹é‡æ·»åŠ çŸ¥è¯†å¤±è´¥: {e}")
            return added_ids

    def batch_delete_lores(self, lore_ids: List[str]) -> int:
        """
        æ‰¹é‡åˆ é™¤çŸ¥è¯† - v2.30.39 æ–°å¢

        Args:
            lore_ids: çŸ¥è¯†IDåˆ—è¡¨

        Returns:
            int: æˆåŠŸåˆ é™¤çš„æ•°é‡
        """
        if self.vectorstore is None:
            return 0

        if not lore_ids:
            return 0

        deleted_count = 0

        try:
            # å»é‡ï¼Œé¿å…é‡å¤ delete é€ æˆé¢å¤–å¼€é”€
            unique_ids = list(dict.fromkeys([str(i) for i in lore_ids if i]))
            if not unique_ids:
                return 0

            previous_records: List[Dict[str, Any]] = []
            try:
                with self._lock:
                    previous_records = self._read_json_records_unlocked()
                    id_set = set(unique_ids)
                    filtered = [lore for lore in previous_records if lore.get("id") not in id_set]
                    deleted_count = len(previous_records) - len(filtered)
                    if deleted_count:
                        self._write_json_records_unlocked(filtered)
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ é™¤çŸ¥è¯†ï¼ˆJSON æ›´æ–°ï¼‰å¤±è´¥: {e}")
                return 0

            # æ‰¹é‡ä»å‘é‡æ•°æ®åº“åˆ é™¤ï¼ˆåœ¨ JSON æˆåŠŸå†™å…¥åæ‰§è¡Œï¼Œå¤±è´¥åˆ™å›æ»š JSONï¼‰
            try:
                self.vectorstore.delete(ids=unique_ids)
            except Exception as e:
                try:
                    with self._lock:
                        self._write_json_records_unlocked(previous_records)
                except Exception:
                    pass
                logger.error(f"æ‰¹é‡åˆ é™¤çŸ¥è¯†ï¼ˆå‘é‡åº“åˆ é™¤ï¼‰å¤±è´¥ï¼Œå·²å›æ»š JSON: {e}")
                return 0

            # æ¸…é™¤ç¼“å­˜
            self._invalidate_cache()

            logger.info(f"æ‰¹é‡åˆ é™¤ {deleted_count} æ¡çŸ¥è¯†")
            return deleted_count

        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ é™¤çŸ¥è¯†å¤±è´¥: {e}")
            return deleted_count

    # ==================== çŸ¥è¯†è´¨é‡ç®¡ç†æ–¹æ³• (v2.30.41 æ–°å¢) ====================

    def provide_feedback(
        self,
        lore_id: str,
        is_positive: bool,
    ) -> bool:
        """
        æä¾›çŸ¥è¯†åé¦ˆ - v2.30.41 æ–°å¢

        Args:
            lore_id: çŸ¥è¯†ID
            is_positive: æ˜¯å¦ä¸ºæ­£é¢åé¦ˆ

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.json_file is None:
            return False

        try:
            updated = False
            with self._lock:
                all_lores = self._read_json_records_unlocked()
                for lore in all_lores:
                    if lore.get("id") == lore_id:
                        if is_positive:
                            lore["positive_feedback"] = lore.get("positive_feedback", 0) + 1
                        else:
                            lore["negative_feedback"] = lore.get("negative_feedback", 0) + 1
                        updated = True
                        self._write_json_records_unlocked(all_lores)
                        break

            if not updated:
                logger.warning(f"æœªæ‰¾åˆ°çŸ¥è¯†: {lore_id}")
                return False

            # æ¸…é™¤ç¼“å­˜ï¼ˆåé¦ˆä¸å½±å“å‘é‡æ£€ç´¢ç»“æœï¼Œä½†ä¼šå½±å“å…ƒæ•°æ®å±•ç¤ºï¼‰
            self._invalidate_cache(reset_hybrid=False, clear_search_cache=False)
            logger.info(f"çŸ¥è¯†åé¦ˆå·²è®°å½•: {lore_id} ({'æ­£é¢' if is_positive else 'è´Ÿé¢'})")
            return True

        except Exception as e:
            logger.error(f"æä¾›åé¦ˆå¤±è´¥: {e}")
            return False

    def assess_knowledge_quality(
        self,
        lore_id: str,
    ) -> Optional[Dict[str, Any]]:
        """
        è¯„ä¼°çŸ¥è¯†è´¨é‡ - v2.30.41 æ–°å¢

        Args:
            lore_id: çŸ¥è¯†ID

        Returns:
            Dict: è¯„ä¼°ç»“æœï¼Œå¤±è´¥è¿”å› None
        """
        if not self.quality_manager:
            logger.warning("çŸ¥è¯†è´¨é‡ç®¡ç†ç³»ç»Ÿæœªå¯ç”¨")
            return None

        try:
            # è·å–çŸ¥è¯†
            all_lores = self.get_all_lores(use_cache=True)
            knowledge = None

            for lore in all_lores:
                if lore.get("id") == lore_id:
                    knowledge = lore
                    break

            if not knowledge:
                logger.warning(f"æœªæ‰¾åˆ°çŸ¥è¯†: {lore_id}")
                return None

            # è¯„ä¼°
            assessment = self.quality_manager.assess_knowledge(
                knowledge, all_lores
            )

            return assessment

        except Exception as e:
            logger.error(f"è¯„ä¼°çŸ¥è¯†è´¨é‡å¤±è´¥: {e}")
            return None

    def get_low_quality_knowledge(
        self,
        threshold: float = 0.3,
    ) -> List[Dict[str, Any]]:
        """
        è·å–ä½è´¨é‡çŸ¥è¯†åˆ—è¡¨ - v2.30.41 æ–°å¢

        Args:
            threshold: è´¨é‡é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è®¤ä¸ºæ˜¯ä½è´¨é‡ï¼‰

        Returns:
            List[Dict]: ä½è´¨é‡çŸ¥è¯†åˆ—è¡¨
        """
        if not self.quality_manager:
            return []

        try:
            all_lores = self.get_all_lores(use_cache=True)
            low_quality = []

            for lore in all_lores:
                quality_score = self.quality_manager.scorer.calculate_quality_score(lore)
                if quality_score < threshold:
                    low_quality.append({
                        **lore,
                        "quality_score": quality_score,
                    })

            # æŒ‰è´¨é‡åˆ†æ•°æ’åº
            low_quality.sort(key=lambda x: x["quality_score"])

            logger.info(f"æ‰¾åˆ° {len(low_quality)} æ¡ä½è´¨é‡çŸ¥è¯†")
            return low_quality

        except Exception as e:
            logger.error(f"è·å–ä½è´¨é‡çŸ¥è¯†å¤±è´¥: {e}")
            return []

    # ==================== çŸ¥è¯†æ¨èæ–¹æ³• (v2.30.42 æ–°å¢) ====================

    def recommend_knowledge(
        self,
        context: Dict[str, Any],
        k: int = 5,
        min_score: float = 0.3,
    ) -> List[Dict[str, Any]]:
        """
        æ¨èçŸ¥è¯† - v2.30.42 æ–°å¢

        Args:
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
                - query: å½“å‰æŸ¥è¯¢
                - topic: å½“å‰ä¸»é¢˜
                - keywords: å…³é”®è¯åˆ—è¡¨
                - recent_topics: æœ€è¿‘è®¨è®ºçš„ä¸»é¢˜
                - user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
            k: æ¨èæ•°é‡
            min_score: æœ€ä½æ¨èåˆ†æ•°

        Returns:
            List[Dict]: æ¨èçš„çŸ¥è¯†åˆ—è¡¨
        """
        if not self.recommender:
            logger.warning("çŸ¥è¯†æ¨èç³»ç»Ÿæœªå¯ç”¨")
            return []

        try:
            # è·å–æ‰€æœ‰çŸ¥è¯†
            all_lores = self.get_all_lores(use_cache=True)

            # æ¨è
            recommendations = self.recommender.recommend(
                context, all_lores, k, min_score
            )

            # è®°å½•ä½¿ç”¨ç»Ÿè®¡
            if self.usage_tracker:
                for rec in recommendations:
                    self.usage_tracker.record_usage(
                        rec.get("id"),
                        context,
                        usage_type="recommendation"
                    )

            logger.info(f"æ¨èçŸ¥è¯†: {len(recommendations)} æ¡")
            return recommendations

        except Exception as e:
            logger.error(f"æ¨èçŸ¥è¯†å¤±è´¥: {e}")
            return []

    def push_knowledge(
        self,
        user_id: str,
        context: Dict[str, Any],
        k: int = 3,
    ) -> List[Dict[str, Any]]:
        """
        ä¸»åŠ¨æ¨é€çŸ¥è¯† - v2.30.42 æ–°å¢

        Args:
            user_id: ç”¨æˆ·ID
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            k: æ¨é€æ•°é‡

        Returns:
            List[Dict]: æ¨é€çš„çŸ¥è¯†åˆ—è¡¨
        """
        if not self.pusher:
            logger.warning("ä¸»åŠ¨æ¨é€ç³»ç»Ÿæœªå¯ç”¨")
            return []

        try:
            # è·å–æ‰€æœ‰çŸ¥è¯†
            all_lores = self.get_all_lores(use_cache=True)

            # æ¨é€
            pushed = self.pusher.push_knowledge(
                user_id, context, all_lores, k
            )

            # è®°å½•ä½¿ç”¨ç»Ÿè®¡
            if self.usage_tracker:
                for knowledge in pushed:
                    self.usage_tracker.record_usage(
                        knowledge.get("id"),
                        context,
                        usage_type="push"
                    )

            logger.info(f"ä¸»åŠ¨æ¨é€çŸ¥è¯†: {len(pushed)} æ¡")
            return pushed

        except Exception as e:
            logger.error(f"ä¸»åŠ¨æ¨é€çŸ¥è¯†å¤±è´¥: {e}")
            return []

    def update_recommendation_preference(
        self,
        user_id: str,
        knowledge: Dict[str, Any],
        is_positive: bool,
    ):
        """
        æ›´æ–°æ¨èåå¥½ - v2.30.42 æ–°å¢

        Args:
            user_id: ç”¨æˆ·ID
            knowledge: çŸ¥è¯†æ¡ç›®
            is_positive: æ˜¯å¦ä¸ºæ­£é¢åé¦ˆ
        """
        if not self.recommender:
            return

        try:
            # æ›´æ–°æ¨èå™¨çš„ç”¨æˆ·åå¥½
            self.recommender.update_user_preference(
                user_id, knowledge, is_positive
            )

            # è®°å½•åé¦ˆç»Ÿè®¡
            if self.usage_tracker:
                self.usage_tracker.record_feedback(
                    knowledge.get("id"),
                    is_positive
                )

            logger.debug(f"æ›´æ–°æ¨èåå¥½: {user_id}, æ­£é¢={is_positive}")

        except Exception as e:
            logger.error(f"æ›´æ–°æ¨èåå¥½å¤±è´¥: {e}")

    def get_knowledge_usage_stats(self, knowledge_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–çŸ¥è¯†ä½¿ç”¨ç»Ÿè®¡ - v2.30.42 æ–°å¢

        Args:
            knowledge_id: çŸ¥è¯†ID

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if not self.usage_tracker:
            return None

        return self.usage_tracker.get_knowledge_stats(knowledge_id)

    def get_top_used_knowledge(self, k: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–æœ€å¸¸ç”¨çš„çŸ¥è¯† - v2.30.42 æ–°å¢

        Args:
            k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: çŸ¥è¯†IDå’Œä½¿ç”¨æ¬¡æ•°åˆ—è¡¨
        """
        if not self.usage_tracker:
            return []

        return self.usage_tracker.get_top_used_knowledge(k)

    def get_unused_knowledge(self, days: int = 30) -> List[str]:
        """
        è·å–æœªä½¿ç”¨çš„çŸ¥è¯† - v2.30.42 æ–°å¢

        Args:
            days: å¤šå°‘å¤©å†…æœªä½¿ç”¨

        Returns:
            List[str]: æœªä½¿ç”¨çš„çŸ¥è¯†IDåˆ—è¡¨
        """
        if not self.usage_tracker:
            return []

        try:
            all_lores = self.get_all_lores(use_cache=True)
            all_ids = [lore.get("id") for lore in all_lores if lore.get("id")]

            return self.usage_tracker.get_unused_knowledge(all_ids, days)

        except Exception as e:
            logger.error(f"è·å–æœªä½¿ç”¨çŸ¥è¯†å¤±è´¥: {e}")
            return []

    def generate_usage_report(self) -> str:
        """
        ç”Ÿæˆä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š - v2.30.42 æ–°å¢

        Returns:
            str: ç»Ÿè®¡æŠ¥å‘Šæ–‡æœ¬
        """
        if not self.usage_tracker:
            return "çŸ¥è¯†ä½¿ç”¨ç»Ÿè®¡ç³»ç»Ÿæœªå¯ç”¨"

        return self.usage_tracker.generate_report()

    # ==================== çŸ¥è¯†å›¾è°±æ–¹æ³• (v2.30.43 æ–°å¢) ====================

    def build_knowledge_graph(self, use_llm: bool = False):
        """
        æ„å»ºçŸ¥è¯†å›¾è°± - v2.30.43 æ–°å¢

        Args:
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM æå–å…³ç³»
        """
        if not self.knowledge_graph:
            logger.warning("çŸ¥è¯†å›¾è°±ç³»ç»Ÿæœªå¯ç”¨")
            return

        try:
            # è·å–æ‰€æœ‰çŸ¥è¯†
            all_lores = self.get_all_lores(use_cache=True)

            # æ„å»ºå›¾è°±
            self.knowledge_graph.build_graph_from_knowledge(
                all_lores,
                use_llm=use_llm
            )

            logger.info("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")

        except Exception as e:
            logger.error(f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")

    def find_related_knowledge_by_graph(
        self,
        knowledge_id: str,
        max_depth: int = 2,
        min_confidence: float = 0.5,
    ) -> List[Dict[str, Any]]:
        """
        é€šè¿‡å›¾è°±æŸ¥æ‰¾ç›¸å…³çŸ¥è¯† - v2.30.43 æ–°å¢

        Args:
            knowledge_id: çŸ¥è¯†ID
            max_depth: æœ€å¤§æ·±åº¦
            min_confidence: æœ€å°ç½®ä¿¡åº¦

        Returns:
            List[Dict]: ç›¸å…³çŸ¥è¯†åˆ—è¡¨
        """
        if not self.knowledge_graph:
            logger.warning("çŸ¥è¯†å›¾è°±ç³»ç»Ÿæœªå¯ç”¨")
            return []

        try:
            related_ids = self.knowledge_graph.find_related_knowledge(
                knowledge_id,
                max_depth,
                min_confidence
            )

            # è·å–å®Œæ•´çš„çŸ¥è¯†ä¿¡æ¯
            all_lores = self.get_all_lores(use_cache=True)
            lore_dict = {lore.get("id"): lore for lore in all_lores}

            related_lores = []
            for rel in related_ids:
                lore_id = rel.get("id")
                if lore_id in lore_dict:
                    lore = lore_dict[lore_id].copy()
                    lore["graph_relation"] = {
                        "relation_type": rel.get("relation_type"),
                        "confidence": rel.get("confidence"),
                        "description": rel.get("description"),
                        "depth": rel.get("depth"),
                    }
                    related_lores.append(lore)

            logger.info(f"é€šè¿‡å›¾è°±æ‰¾åˆ° {len(related_lores)} ä¸ªç›¸å…³çŸ¥è¯†")
            return related_lores

        except Exception as e:
            logger.error(f"é€šè¿‡å›¾è°±æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†å¤±è´¥: {e}")
            return []

    def find_knowledge_path(
        self,
        source_id: str,
        target_id: str,
    ) -> Optional[List[Dict[str, Any]]]:
        """
        æŸ¥æ‰¾ä¸¤ä¸ªçŸ¥è¯†ä¹‹é—´çš„è·¯å¾„ - v2.30.43 æ–°å¢

        Args:
            source_id: æºçŸ¥è¯†ID
            target_id: ç›®æ ‡çŸ¥è¯†ID

        Returns:
            Optional[List[Dict]]: è·¯å¾„ï¼ˆçŸ¥è¯†åˆ—è¡¨ï¼‰ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if not self.knowledge_graph:
            logger.warning("çŸ¥è¯†å›¾è°±ç³»ç»Ÿæœªå¯ç”¨")
            return None

        try:
            path_ids = self.knowledge_graph.find_path(source_id, target_id)

            if not path_ids:
                return None

            # è·å–å®Œæ•´çš„çŸ¥è¯†ä¿¡æ¯
            all_lores = self.get_all_lores(use_cache=True)
            lore_dict = {lore.get("id"): lore for lore in all_lores}

            path_lores = []
            for lore_id in path_ids:
                if lore_id in lore_dict:
                    path_lores.append(lore_dict[lore_id])

            logger.info(f"æ‰¾åˆ°çŸ¥è¯†è·¯å¾„: {len(path_lores)} ä¸ªèŠ‚ç‚¹")
            return path_lores

        except Exception as e:
            logger.error(f"æŸ¥æ‰¾çŸ¥è¯†è·¯å¾„å¤±è´¥: {e}")
            return None

    def infer_new_relations(self, knowledge_id: str) -> List[Dict[str, Any]]:
        """
        æ¨ç†æ–°çš„çŸ¥è¯†å…³ç³» - v2.30.43 æ–°å¢

        Args:
            knowledge_id: çŸ¥è¯†ID

        Returns:
            List[Dict]: æ¨ç†å‡ºçš„æ–°å…³ç³»åˆ—è¡¨
        """
        if not self.knowledge_graph:
            logger.warning("çŸ¥è¯†å›¾è°±ç³»ç»Ÿæœªå¯ç”¨")
            return []

        try:
            inferences = self.knowledge_graph.infer_knowledge(knowledge_id)
            logger.info(f"æ¨ç†å‡º {len(inferences)} æ¡æ–°å…³ç³»")
            return inferences

        except Exception as e:
            logger.error(f"æ¨ç†æ–°å…³ç³»å¤±è´¥: {e}")
            return []

    def get_graph_statistics(self) -> Dict[str, Any]:
        """
        è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯ - v2.30.43 æ–°å¢

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.knowledge_graph:
            return {"error": "çŸ¥è¯†å›¾è°±ç³»ç»Ÿæœªå¯ç”¨"}

        return self.knowledge_graph.get_statistics()

    def export_graph_for_visualization(self) -> Dict[str, Any]:
        """
        å¯¼å‡ºå›¾è°±æ•°æ®ç”¨äºå¯è§†åŒ– - v2.30.43 æ–°å¢

        Returns:
            Dict: å¯è§†åŒ–æ•°æ®
        """
        if not self.knowledge_graph:
            return {"error": "çŸ¥è¯†å›¾è°±ç³»ç»Ÿæœªå¯ç”¨"}

        return self.knowledge_graph.export_for_visualization()
